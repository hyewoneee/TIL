# 도서리뷰 Real World HTTP 리얼 월드 HTTP
책을 읽고 정리하는 공간

# 목차

- [x] 1장. HTTP/1.0의 신택스: 기본이 되는 네 가지 요소
- [x] 2장. HTTP/1.0의 시맨틱스: 브라우저 기본 기능의 이면
- [x] 3장. GO 언어를 이용한 HTTP/1.0 클라이언트 구현
- [X] 4장. HTTP/1.1의 신택스: 고속화와 안전성을 추구한 확장
- [x] 5장. HTTP/1.1의 시맨틱스: 확장되는 HTTP의 용도
- [x] 6장. Go 언어를 이용한 HTTP1.1 클라이언트 구현
- [] 7장. HTTP/2의 신택스: 프로토콜 재정의
- [] 8장. HTTP.2의 시맨틱스: 새로운 활용 사례
- [] 9장. Go 언어를 이용한 HTTP/2, HTTP5 프로토콜 구현
- [] 10장. 보안: 브라우조를 보호하는 HTTP의 기능
- [] 11장. 클라이언트 시점에서 보는 RESTful API

# 1장. HTTP/1.0의 신택스: 기본이 되는 네 가지 요소

HTTP를 데이터의 상자로서 보면 통신 내용을 몇 가지 요소로 나눌 수 있다.

이 장에서는 다음 네 가지 기본 요소에 초점을 맞춰 소개한다.

- 메서드와 경로
- 헤더
- 바디
- 스테이터스 코드

HTTP를 다루는 도구로서 curl 커맨드 사용법도 소개한다.

curl 커맨들르 사용하면 원하는대로 서버에 요청을 보낼 수 있다.

### HTTP/0.9에서 1.0으로의 여정

HTTP/0.9는 매우 단순했지만 `'브라우저가 문서를 요청하면, 서버는 데이터를 반환한다'` 라는 웹의 기본 뼈대는 이미 이 시점에서 완성됐다.

그렇지만 이 프로토콜로는 할 수 없는 일이 많았다.

- 하나의 문서를 전송하는 기능밖에 없었다.
- 통신되는 모든 내용은 HTML 문서로 가정했으므로, 다운로드할 콘텐츠의 형식을 서버가 전달할 수단이 없었다.
- 클라이언트 쪽에서 검색 이외의 요청을 보낼 수 없었다.
- 새로운 문장을 전송하거나 갱신 또는 삭제할 수 없었다.

그 밖에도 요청이 올바른지 혹은 서버가 올바르게 응답했는지 아는 방법도 없었다.

1992년 버전에서는 단순버전(0.9 호환모드)과 전 기능(1.0과 거의 같다) 버전으로 두 종류의 요청 형식이 있었다.

요청의 변경된 점은 다음과 같다.

- 요청 시 메서드가 추가됐다(GET).
- 요청 시 HTTP 버전이 추가됐다(HTTP/1.0).
- 헤더가 추가됐다(Host, User-Agent, Accept).

### HTTP의 조상(1) 전자메일

HTTP에도 이 전자메일과 똑같은 형식의 헤더가 도입됐다.

헤더는 서버와 클라이언트 사이에 필요한 추가 정보, 지시나 명령, 당부 등을 쓰는 장소다.

우선 클라이언트가 서버에 보내는 헤더다

**User-Agent**

클라이언트가 자신의 애플리케이션에 이름을 넣는곳.

curl 커맨드를 사용하면 curl/7.48.0과 같은 문자열이 들어가며

서버는 이곳의 이름을 보고 응답을 전환하기도 한다.

**Referer**

서버에서  참고하는 추가정보

클라이언트가 요청을 보낼 때 보고 있던 페이지의 URL을 보낸다.

페이지의 참조원을 서버가 참조하는데 이용한다.

**Authorization**

특별한 클라이언트에만 통신을 허가할 때 인증 정보를 서버에서 전달한다.

RFC에서 몇 가지 표준 형식(Basic/Digest/Bearer)을 정했지만, 아마존 웹 서비스나 깃허브 API 등에서는 웹 서비스 자체 표기를 요구하기도 한다.

서버에서 클라이언트로 보낼 때 부여하는 헤더는 다음과 같은 것이 있다.

**Content-Type**

파일 종류를 지정

여기에는 MIME 타입이라는 식별자를 기술한다.

MIME 타입은 전자메일을 위해 만들어진 식별자다.

**Content-Length**

바디크기

**Content-Encoding**

압축이 이루어진 경우 압축 형식을 설명

**Date**

문서 날짜

### MIME 타입

MIME 타입은 파일의 종류를 구별하는 문자열로, 전자메일을 위해 만들어졌다.

파일 종류에 따라 브라우저 화면에 표시하거나 `'저장'` 대화창을 표시하는 기능을 제공하는데,

이때 파일 종류를 나타내는 식별자가 MIME 타입이다.

### Content-Type과 보안

브라우저 세계에서는 앞에서 소개한 것처럼 파일 종류를 특정할 때 Content-Type 헤더에서 지정된 MIME 타입을 사용 

인터넷 익스플로러는 인터넷 옵션에 따라 MIME 타입이 아닌 내용을 보고 파일 형식을 추측하려고 한다.

이런 동작을 `콘텐트 스피닝(content sniffing)` 이라 한다.

이때 서버 설정이 잘못된 경우에도 제대로 표시되므로 얼핏 사용자에게 장점이 있는것처럼 여겨질 수 있지만, 원래 텍스트로만 표시돼야 하는 `text/plain` 파일인데도 HTML과 자바스크립트가 적혀있으면 브라우저가 파일을 실행해버리는 일도 있다.

뜻밖에 보안의 구멍이 될 수 있다.

서버에서는 다음과 같은 헤더를 전송해 브라우저가 추측하지 않도록 지시하는 것이 현재 주류의 방법이다.

`X-Content-Type-Option: nosniff`

### 전자메일과의 차이

헤더를 설명하면서 전자메일 포맷을 소개했다. HTTP와 비교해보자

- '헤더' + '본문' 구조는 같다.
- HTTP 요청에서는 선두에 '메서드+패스' 행이 추가된다.
- HTTP  응답에서는 선두에 스테이터스 코드가 추가된다.

### 메서드

 HTTP/1.0으로 통신할 때 전송되는 GET 부분은 메서드로 불린다.

- GET : 서버에 헤더와 콘텐츠 요청
- HEAD: 서버에 헤더만 요청
- POST: 새로운 문서 투고
- PUT: 이미 존재하는 URL의 문서를 갱신
- DELETE: 지정된 URL의 문서를 삭제

curl 커멘드로 메서드를 전송할 때는 `--request=` 메서드 혹은 그 단축형인 `-X` 메서드를 사용

	curl --http1.0 -X POST http://localhost:18888/greeting

### 스테이터스 코드

**100번대**

처리가 계속됨을 나타낸다

1xx계열은 특수한 용도로 사용

**200번대**

성공했을 때의 응답

**300번대**

서버에서 클라이언트로의 명령

오류가 아니라 정상 처리의 범주

리디렉트나 캐시 이용을 지시

**400번대**

클라이언트가 보낸 요청에 오류가 있다.

**500번대**

서버 내부에서 오류가 발생

### 리다이렉트

300번대 스테이터스의 일부는 서버가 브라우저에 대해 리디렉트하도록 지시하는 스테이터스 코드다.

300 이외의 경우는 **Location** 헤더를 사용해 리디렉트할 곳을 서버에서 클라이언트로 전달

영구적인지 일시적인지는 이동하는 이전 페이지가 이후에도 존재하는지로 분류한다.

**영구적**

새 도메인을 얻어 서버의 콘텐츠를 이동한 경우나 HTTP로 운영되던 페이지를 HTTPS로 전환한 경우에는 에전 페이지를 볼 일이 없다.

**일시적**

점검 기간에만 요청을 관리 화면으로 리디렉트할 경우 점검이 끝나면 복구해 다시 활성화할 것

**301/308**

요청된 페이지가 다른 장소로 이동했을 때 사용

영구적으로 이동

검색 엔진도 이 응답을 받으면 기존 페이지의 평가를 새로운 페이지로 게승한다.

구글은 검색 엔진에 페이지 이동을 전하는 수단으로써 **301**을 사용할 것을 권장

**302/307**

일시적인 이동

모바일 전용 사이트로 이동하거나 관리 페이지를 표시

**303**

요청된 페이지에 반환할 콘텐츠가 없거나 혹은 원래 반환할 페이지가 따로 있을 때, 그쪽으로 이동시키려고 사용한다.(로그인 페이지를 사용해 로그인한 후 원래 페이지로 이동하는 경우에 사용)

클라이언트는 **Location** 헤더 값을 보고, 다시 요청한다

재전송할 때는 헤더 등도 다시 보낸다.

curl 커맨드에 `-L` 을 부여하면, 응답이 300번대고 게다가 응답 헤더에 **Location** 헤더가 있으면 그 헤더에서 지정된 URL에 다시 요청을 보낸다.

또한 스테이터스 코드가 **301, 302, 303**이고 **GET** 이외의 메서드인 경우에는 **GET**으로 리디렉트를 다시 보낸다.

메서드를 바꿀수 없게 하는 옵션(—post301, —post302, —post303)도 있다.

기본으로 최대 50번 까지 리디렉트 한다.

리디렉트 횟수도 `--max-redirs` 옵션으로 지정할 수 있다.


### URL의 구조

URL은 아래와 같은 요소로 구성됨

**스키마://호스트명/경로**

- 스키마: https
- 호스트명: www.oreilly.co.jp
- 경로: index.shtml

URL 사양에 포함되는 모든 요소가 들어간 예제는 다음과 같은 형식

**스키마://사용자:패스워드@호스트명:포트/경로#프래그먼트?쿼리**

자주 보는 스키마로는 http외에 통신 경로가 암호화 되는 https, 메일러를 시작하는 mailot가 있다.

브라우저는 스키마를 보고 적절한 접속 방법을 선택해야한다.

실제로 통신하는 곳은 **호스트명**으로 지정된 서버이다.

포트는 아파트 등의 현관 우편함 같은 것이다.

주소(호스트명으로 찾아온 IP주소) 마다 65,535개의 포트가 있다.

같은 주소라도 포트가 다르면 독립적으로 복수의 서버를 운영해 서비스를 제공할 수 있다.

포트가 새얅되면 스키마별 기본 포트를 사용

HTTP : 80번 포트

HTTPS: 443번 포트

프래그먼트는 HTML에서는 페이지 내 링크의 앵커를 지정하는데 쓰인다.

쿼리는 검색 용어를 지정하거나 포시하고 싶은 웹페이지에 대해서 특정 파라미터를 부여하는데 사용 

URL은 주소를 지정하는 데 사용하지만, 동시에 `'사용자가 읽는 문장'`  이기도 하다.

### 바디

HTTP/0.9 사양에서는 요청에 데이터를 포함할 수 없었다.

응답은 파일 콘텐츠 자체였지만, 1.0에서는 요청과 응답 양쪽에 헤더가 포함돼 바디와 헤더를 분리할 필요가 있다. 또한 요청에도 콘텐츠를 포함할 수 있게 돼 새로운 역할이 늘어났다.

헤더 끝에 빈 줄을 넣으면 그 이후는 모두 바디가 된다.

이 구조는 전자메일과 똑같지만, 전송할 때 데이터를 저장하는 포맷이 두 종류로 용도에 맞게 구분할 필요가 있다.

    헤더1: 헤더 값 1
    헤더2: 헤더 값 2
    Content-Length: 바디의 바이트 수
    
    여기서부터 지정된 바이트 수만큼 바디가 포함된다.

한 번 응답할 때마다 한 파일만 반환하기 때문에 HTTP에서 응답의 바디는 단순하다.

속도를 위해 바디를 압축하는 경우가 있다.

이때는 Content-Encoding에서 지정된 압축 알고리즘으로 읽어 온 바디의 데이터를 전개할 필요가 있다.

이 경우 Content-Length는 압축 전 콘텐츠 길이가 아니라 압축 후 통신 데이터 크기다.

서버와 통신이 확립된 소켓에서 클라이언트가 읽는 바이트 수는 Content-Length에 적힌 데이터 길이로 압축되지 않았을 때와 다르지 않다.
---
# 2장. HTTP/1.0의 시맨틱스: 브라우저 기본 기능의 이면

이 장에서는 curl 커맨드를 이용해 브라우저의 동작을 이해한다.

### 단순한 폼 전송(x-www-form-urlencoded)

curl 커맨드의 `-d` 옵션을 사용해 폼으로 전송할 데이터를 설정할 수 있다.

curl 커맨드는 `-d` 옵션이 지정되면 브라우저와 똑같이 헤더로 Content-Type:application/x-www-form-rulencoded를 설정

`-d` 옵션으로 보낼 경우 지정된 문자열을 그대로 연결

구분문자인 `&` 와 `=` 이 있어도 그대로 연결해버리므로, 읽는 쪾에서 바르게 원래 데이터 세트로 복원할 수 없다.

에를 들어 `Head First PHP & MYSQL` 이라는 서적명을 넣어보면, 어디서 구분해야 할 지 알기 어려워진다.

### 폼을 이용한 파일 전송

HTML의 폼에서는 옵션으로 멀티파트 폼 형식이라는 인코딩 타입을 선택할 수 있다.

보통 HTTP 응답은 한 번에 한 파일씩 반환하므로, 빈 줄을 찾아 그곳부터 `Content-Length` 로 지정된 바이트 수만큼 읽기만 하면 데이터를 통째로 가져올 수 있다.(파일의 경계를 신경쓸 필요가 없다)

멀티파트를 이용하는 경우는 한 번의 요청으로 복수의 파일을 전송할 수 있으므로 받는 쪽에서 파일을 나눠야한다.

`-d` 대신에 `-F` 를 사용하는 것만으로 curl 커맨드는 `enctype="multipart/form-data"`가 설정된 폼과 같은 형식으로 송신한다.

`-d` 와  `-F` 를 섞어 쓸 수는 없다. 

파일 전송은 `@` 를 붙여 파일 이름을 지정하면, 그 내용을 읽어와서 첨부한다.

아래와 같이 전송할 파일명과 파일 형식을 수동으로 설정할 수 있다.

type과 filename은 동시에 설정할 수 있다.

	# 파일 내용을 test.txt에서 최득. 파일명은 로컬 파일명과 같다. 형식도 자동 설정
	curl --http1.0 -F attachment-file@test.txt http://localhost:18888
	
	# 파일 내용을 test.txt에서 취득. 형식은 수동으로 지정.
	curl --http1.0 -F "attachment-file@test.txt;type=text/html" http://localhost.18888
	
	# 파일 내용을 test.txt에서 취득. 파일명은 지정한 파일명을 이ㅛㅇㅇ.
	curl --http1.0 -F "attachment-file@test.txt;filename-sample.txt"
	http://localhost:18888

### 폼을 이용한 리디렉트

1장의 1.6 리디렉트에서는 300번대 스테이터스 코드를 사용한 리디렉트를 소개했다.

하지만 이 방법에는 몇 가지 제한이 있다.

- 1장의 URL 항목에서 설명한 것처럼 URL에는 2천 자 이내라는 기준이 있어, GET의 쿼리로 보낼 수 있는 데이터양에 한계가 있다.
- 데이터가 URL에 포함되므로, 전송하는 내용이 액세스 로그 등에 남을 우려가 있따.

이런 분제를 피하고자 종종 이용되는 방법이 HTML의 폼을 이용한 리디렉트다.

서버로 부터는 리디렉트할 곳으로 보내고 싶은 데이터가 `<input type="hidden">` 태그로 기술된 HTML이 되돌아 온다. 

폼에서 보내는 곳이 리디렉트 할 곳이다.

브라우저가 이 HTML을 열면, 로드 직후 발생하는 이벤트로 폼을 전송하므로 즉시 리디렉트해 이동하게 된다.

이 방법의 장점은 인터넷 익스플로러에서도 데이터양에 제한이 없다는 점이다.

단점으로는 순간적으로 빈 페이지가 표시된다는 것과 전환 버튼이 표시되긴 하지만 자바스크립트가 비활성화되어 있으면 자동으로 전환되지 않는다는 점이다.

### 콘센트 니고시에이션

콘센트 니고시에이션은 통신 방법을 최적화하고자 하나의 요청 안에서 서버와 클라이언트가 서로  최고의 설정을 공유하는 시스템이다.

콘센트 니고시에이션에는 헤더를 이용한다.

	요청 헤더            응답                              니고시에이션 대상						
	Accept             Content-Type 헤더                 MIME 타입
	Accept-Language    Content-Language 헤더/html 태그    표시언어
	Accept-Charset     Content-Type 헤더                 문자의 문자셋
	Accept-Encoding    Content-Encoding 헤더             바디 압축

### 압축을 이용한 통신 속도 향상

콘텐츠 압축은 전송 속도 향상을 위한 것이다.

콘텐츠 내용에 따라 다르지만, 현재 일반적으로 사용되는 압축 알고리즘을 적용하면 텍스트 파일은 1/10 크기로 압축된다.

같은 기호가 반복해서 나오는 JSON 이라면 1/20 정도로 압축 할 수 있다.

통신에 걸리는 시간보다 압축과 해제가 짧은 시간에 이루어지므로, 압축을 함으로써 웹 페이지를 표시할 때 걸리는 전체적인 처리 시간을 줄일 수 있다.

콘텐츠 압축은 전송 속도 향상뿐만 아니라 이용 요금에도 영향을 미친다.

콘텐츠를 압축하면 비용부다도 줄어들고 모바일 단말은 전파 송수신에 전력을 많이 소비하므로, 전력 소비가 줄어드는 효과도 기대할 수 있다.

콘텐츠 압축 니고시에이션은 모두 HTTP의 헤더 안에서 완료한다.

우선 클라이언트가 수용 가능한 압축방식을 헤더에서 지정한다.

여기에서는 `deflate` 와 `gzip` 두가지를 지정햇다.

`Accept-Encoding: deflate, gzip`

curl 커맨드에서 `--compressed` 옵션을 지정하면 , `-H` 옵션으로 위 헤더를 기술한 것과 같다

	curl --http1.0 --compressed http://localhost:18888

서버는 전송받은 목록 중 지원하는 방식이 있으면, 응답할 때 그 방식으로 압축하거나 미리 압축된 콘텐츠를 반환한다.

서버가  `gzip` 을 지원하면, 조금 전에 받은 요청에 대한 응답으로 다음과 같은 헤더가 부여된다.

콘텐츠의 데이터양을 나타내는 `Content-Lenhth` 헤더는 압축된 파일 크기다.

`Content-Encoding: gzip` 

서버에서 클라이언트로 첫 번째 웹 페이지를 반환할 대 `Accept-Encoding`헤더를 부여하고, 그런 다음 클라이언트에서 무언가 업로드 할 때 `Content-Encoding` 을 부여한다.

지금의 고속화 방식과는 대조적으로 헤더가 이용된다. 

요청, 응답 양쪽에서 똑같이 헤어 구조가 이용되므로 이처럼 간단하게 구현할 수 있다.

### 쿠키

쿠키란 웹 사이트의 정보를 브라우저 쪽에 저장하는 작은 파일이다.

서버가 클라이언트(브라우저)에 `'이 파일 보관해줘'` 라고 쿠키 저장을 지시한다.

쿠키도 HTTP 헤더를 기반으로 구현됐다.

서버에서는 다음과 같이 응답 헤더를 보낸다.

	Set-Cookie: LAST_ACCESS_DATE=Jul/31/2016
	Set-Cookie: LAST_ACCESS_TIME=12:04

각각 `이름=값` 형식으로 회신했는데, 클라이언트는 이 값을 저장해 둔다.

다음번에 방문할 때는 다음과 같은 형식으로 보내자.

서버는 이 설정을 읽고,  클라이언트가 마지막으로 액세스한 시간을 알 수 있다.

	Cookie: LAST_ACCESS_DATE=Jul/31/2016
	Cookie: LAST_ACCESS_TIME=12:04

브라우저에서도 자바스크립트로 쿠키를 읽어내거나 서버에 보낼 때 쿠키를 설정할 수 있다.

개발자 도구를 열고 `document.cookie` 속성을 보면 쿠키가 문자열 형식으로 들어있는 것을 알 수 있다.

쿠키는 헤더를 바탕으로 만들어졌으므로 curl 커맨드를 사용할 때도 헤더로서 받은 내용을  `Cookie`에 넣고 재전송함으로써 실현할 수 있지만,

쿠키를 위한 전용 옵션도 있다.

`-c/--cookie-jar` 옵션으로 지정한 파일에 수신한 쿠키를 지정하고

`-b/--cookie` 옵션으로 지정한 파일에서 쿠키를 읽어와 전송한다.

브라우저처럼 동시에 송수신하려면 둘 다 지정해야한다.

`-b/--cookie` 옵션은 파일에서 읽기만 하는 게 아니라 개별 항목을 추가할 때도 사용할 수 있다.

	curl --http1.0 -c cookie.txt -b cookie.txt -b "name=value"
	http://example.com/helloworld

### 쿠키의 잘못된 사용법

쿠키는 편리한 기능이지만, 몇 가지 제약이 있어 적절하지 않은 사용법이 있다.

우선 영속성 문제

쿠키는 어떤 상황에서도 확실하게 저장되는 것은 아니다.

비밀모드 혹은 브라우저의 보안 설정에 따라 세션이 끝나면 초기화 되거나 쿠키를 보관하라는 서버의 지시를 무시하기도 한다.

서버가 쿠키를 데이터베이스 대신으로 쓸 수 는 없다.

쿠키가 초기화 되면 저장된 데이터는 사라지기 때문에 사라지더라도 문제가 없는 정보나 서버 정보로 복원할 수 있는 자료를 저장하는 용도에 적합하다.

또한 용량문제도 있다.

쿠키의 최대 크기는 4킬로바이트 사양으로 정해져 있어 더 보낼 수 는 없다.

쿠니는 헤더로서 항상 통신에 부가되므로 통신량이 늘어나는데, 통신량 증가는 요청과 응답 속도 모두에 영향을 미친다.

제한된 용량과 통신량 증가는 데이터베이스로 사용하는데 제약이 된다.

보안문제

`secure` 속성을 부여하면 HTTPS 프로토콜로 암호화된 통신에서만 쿠키가 전송되지만,

HTTP 통신에서는 쿠키가 평문으로 전송된다.

매 요청 시 쿠키가 송수신되는데, 보여선 안되는 정보등이 포함되면 노출될 위험성이 있다.

암호화된다고해도 사용자가 자유롭게 접근할 수 있는 것도 문제이다.

원리상 사용자가 쿠키를 수정할수도 있으므로, 시스템에서 필요한 정보가 수정되면 오작동으로 이어지는 민감한 정보를 넣는 것도 적합하지 않다. 

정보를 넣을 때는 서명이나 암호화처리가 필요하다.

기본적으로는 인증 정보나 사라져도 문제가 없는 정보만 쿠키에 넣는 편이 좋다.

### 쿠키에 제약을 주다

클라이언트는 서버가 보낸 쿠키를 로컬 스토리지에 저장하고, 같은 URL로 접속할 때 저장된 쿠키를 읽고 요청 헤더에 넣는다.

HTTP 클라이언트는 이 속성을 해석해 쿠키 전송을 제어할 책임이 있다.

속성은 세미콜론으로 구분하여 나열한다.

속성은 대문자와 소문자를 구별하지 않으므로 모두 소문자로 써도 유효하다

**Expires, Max-Age 속성**

쿠키의 수명을 설정

Max-Age는 초 단위로 지정.

현재 시각에서 지정된 초수를 더한 시간에서 무효가 된다.

**Domain 속성**

클라이언트에서 쿠키를 전송할 대상 서버

쿠키를 발행한 서버가 된다.

**Path 속성**

클라이언트에서 쿠키를 전송할 대상 서버의 경로

쿠키를 발행한 서버의 경로

**Secure 속성**

https로 프로토콜을 사용한 보안 접속일 때만 클라이언트에서 서버로 쿠키를 전송

쿠키는 URL을 키로 전송을 결정하므로, DNS 해킹으로 URL을 사칭하면 의도치 않은 서버에 쿠키를 전송할 위험이 있다.

DNS 해킹은 기기를 조작하지 않고도 무료 와이파이 서비스 등으로 속여 간단히 할 수 있다.

Secure 속성을 붙이면 http 접속일 때는 브라우저가 경고를 하고 접속하지 않아 정보 유출을 막게 된다.

**HttpOnly 속성**

쿠키를 소개할 때 쿠키를 자바스크립트로 다룰 수 있다고 설명했지만, 이 속성을 붙이면 자바스크립트 엔진으로부터 쿠키를 감출 수 있다.

크로스 사이트 스크립팅등 악의적인 자바스크립트가 실행되는 보안 위험에 대한 방어가 된다.

**SameSite 속성**

이 속성은 RFC에는 존재하지 않는다.

크롬 브라우저 버전 51에서 도입한 속성으로, 같은 오리진(출저)의 도메인에 전송된다.

### 인증과 세션

인증에는 몇 가지 방식이 있다.

유저명과 패스워드를 매번 클라이언트에서 보내는 방식 두 가지를 먼저 소개한다.

### BASIC 인증과 Digest 인증

가장 간단한 것이 BASIC 인증이다.

BASIC 인증은 유저명과 패스워드를 BASE64로 인코딩 한 것

BASE64 인코딩은 가역변환이므로 서버로부터 복원해 원래 유저명과 패스워드를 추출할 수 있다.

추출된 정보를 서버의 데이터베이스와 비교해서 정상 사용자인지 검증

단 SSL/TLS 통신을 사용하지 않은 상태에서 통신이 감청되면 손쉽게 로그인 정보가 유출된다.

BASIC보다 강력한 Digest 인증이다.

Digest 인증은 해시 함수(A→B는 쉽게 계산할 수 있지만, B→A는 쉽게 계산할 수 없다)를 이용한다.

브라우저가 보호된 영역에 접속하려고 하면, **401 Unauthrized**라는 스테이터스 코드로 응답이 돌아온다.

이때 아래와 같은 헤더가 부여된다

	WWW-Authenticate: Digest realm="영역명", nonce="1234567890", algorith=MD5, qop="auth"

`realm` 은 보호되는 영역의 이름으로, 인증창에 표시된다.

`nonce`는 서버가 매번 생성하는 랜덤한 데이터다

`qop` 는 보호 수준을 나타낸다.

클라이언트는 이곳에서 주어진 값과 무작위로 생성한 `cnonce` 를 바탕으로 다음처럼 계산해서 response를 구한다.

클라이언트에서는 생성한 cnonce와 계산으로 구한 response를 부여해 한데 모으고, 다음과 같은 헤더를 덧붙여 재요청을 보낸다.

서버측에서도 이 헤더에 있는 정보와 서버에 저장된 유저명, 패스워드로 같은 계산을 실시한다.

재발송된 요청과 동일한 response가 계산되면 사용자가 정확하게 유저명과 패스워드를 입력했음을 보증할 수 있다.

이로써 유저명과 패스워드 자체를 요청에 포함하지 않고도 서버에서 사용자를 올바르게 인증할 수 있게 된다.

### 쿠키를 사용한 세션 관리

지금은 BASIC 인증과 Digest 인증 모두 많이 사용되지 않는다.

- 특정 폴더 아래를 보여주지 않는 방식으로만 사용할 수 있어, 톱페이지에 사용자 고유 정보를 제공할 수 없다.
- 톱페이지에 사용자 고유 정보를 제공하려면 톱페이지도 보호할 필요가 있어, 톱페이지 접속과 동시에 로그인 창을 표시해야한다. 처음 방문하는 사용자에게 친절한 톱페이지는 아니다.
- 요청할 때마다 유저명과 패스워드를 보내고 계산해서 인증할 필요가 있다. 특히 Digest 인증 방식은 께산량도 많다.
- 로그인 화면을 사용자화할 수 없다. 최근에는 피싱 대책으로 (미리 가지고 있는) 사용자 ID에 대응하는 이미지를 표시하는 등 가짜 사이트가 아님을 사용자가 인지할 수 있는 시스템을 제공하는 사이트가 있따.
- 명시적인 로그오프를 할 수 없다.
- 로그인한 단말을 식별할 수 없다. 게임등 동시 로그인을 막고 싶은 서비스나 구글처럼 미등록 단말로 로그인할 때 보안 경고를 등록된 메일로 보내는 기능이 있는 웹 서비스도 있다.

최근 많이 사용되는 방식은 폼을 이용한 로그인과 쿠키를 이용한 세션 관리를 조합이다.

클라이언트는 폼으로 ID와 비밀번호를 전송한다.

Digest 인증과 달리, 유저 ID와 패스워드를 직접 송신하므로 SSL/TLS이 필수다.

서버 측에서는 유저 ID와 패스워드로 인증하고 문제가 없으면 세션 토큰을 발행한다.

서버는 세션 토큰을 관계형 데이터베이스나 키 밸류형 데이터베이스에 저장해둔다.

토큰은 쿠키로 클라이언트에 되돌아간다.

두 번째 이후 접속에서는 쿠키를 재전송해서 로그인된 클라이언트임을 서버가 알 수 있다.

### 서명된 쿠키를 이용한 세션 데이터 저장

쿠키는 통신량을 증가시키므로 조심하자고 설명했지만, 원래의 용ㅗ대로 스토리지로서 사용할 수 있다.

웹 애플리케이션 프레임워크는 영속화 데이터를 읽고 쓰는 OR 매퍼 등의 시스템과 함께 휘발서 높은 데이터를 다루는 세션 스토리지 기능을 갖추고 있다.

통신 속도가 빨라지고 웹사이트 자체의 데이터양도 많이 늘어나면서, 쿠키의 데이터양 증가는 걱정할 필요가 없어졌다.

그래서 쿠키를 사용한 데이터 관리 시스템도 널리 사용되기 시작했다.

시스템에서 변조되지 않도록 클라이언트에 전자 서명된 데이터를 보낸다.

클라이언트가 서버로 쿠키를 재전송하면서 서버는 서명을 확인한다.

서명하는 것도 서명을 확인하는 것도 서버에서 하므로, 클라이언트는 열쇠를 갖지 않는다.

공개 키와 비밀 키 모두 서버에 있다.

이 시스템의 장점은 서버 측에서 데이터저장 시스템을 준비할 필요가 없다.

서버를 상세하게 기능단위로 나누는 마이크로서비스라도 세션 스토리지 암호화 방식을 공통화해두면 따로 데이터스토어를 세우지 않고 세션 데이터를 읽고 쓸 수 있게 된다.

클라이언트 입장에서 보면 서버에 액세스해서 조작한 결과가 쿠키로 저장된다.

쿠키를 갖고 있는 한 임시 데이터가 유지된다.

다만 전통적인 맴캐시드(mamcached)라든지 관계형 데이터베이스를 이용하는 세션 스토리지와 달리, 같은 사용자라도 스마트폰과 컴퓨터로 각각 접속한 경우 데이터가 공유되지 않는다.

### 프록시

**프록시**

- 통신 내용을 이해한다.
- 필요에 따라서 콘텐츠를 수정하거나 서버 대신 응답한다.

**게이트웨이**

- 통신 내용을 그대로 전송한다.
- 내용의 수정도 불허한다.
- 클라이언트에서는 중간에 존재하는 것을 알아채서는 안된다.

프록시는 HTTP등의 동신을 중계한다.

때로는 각 종 부가 기능을 구현한 경우도 있다.

- 캐시 기능이 있는 프록시를 조직의 네트워크 출입구에 설치하면, 콘텐츠를 저장한 웹 서버의 부담은 줄이고 각 사용자가 페이지를 빠르게 열람할 수 있게 하는 효과가 있다.
- 외부 공격으로부터 네트워크를 보호하는 방화벽 역할
    - 저속 통신 회선용으로 데이터를 압축해(이미지 화질은 떨어진다) 속도를 높이는 필터나 콘텐츠 필터링 등에도 프록시가 이용된다.

프록시 구조는 단순해서 `GET`  등의 메서드 다음에 오는 경로명 형식만 바뀐다.

프록시를 설정하면 스키마가 추기돼, `http://` 나 `https://` 로 시작되는 URL 형식이 된다.

프록시용 통신은 중계할 곳으로 요청을 리디렉트하고 결과를 클라이언트에 반환한다.

프록시 서버가 악용되지 않도록 인증을 이용해 보호하는 경우가 있다

이런 경우는 `Proxy-Authenticate` 헤더에 인증 방식에 맞는 값이 들어간다.

중계되는 프록시는 중간의 호스트 IP 주소를 특정 헤더에 기록해 간다.

 소개한다.

## 캐시

콘텐츠가 변경되지 않았을 땐 로컬에 저장된 파일을 재사용함으로써 다운로드 횟수를 줄이고 성능을 높이는 `캐시` 매너니즘이 등장했다.

### Expires

통신 자체를 없애는 방법이 HTTP/1.0에 도입되었으며, **Expires** 헤더를 이용한다.

`Expires` 헤더에는 날짜와 시간이 들어간다.

클라이언트는 지정한 기한 내라면 캐시가 `신선` 하다고 판단해 강제로 캐시를 이용한다.(요청을 아예 전송하지 않는 것, 캐시의 유효기간이 지났으면 캐시가 신선하지 않다고 판단)

`3초 후 콘텐츠 유효 기간이 끝난다` 라고 설정했어도 3초 후에 마음대로 리로드하지는 않는다.

여기에 설정된 날짜와 시간은 어디까지나 접속을 할지 말지 판단할 때만 사용한다.

또한 `뒤로 가기 버튼`등으로 방문이력을 조작하는 경우는 기한이 지난 오래된 콘텐츠가 그대로 이용될 수 있다.

### Pragma: no-cache

클라이언트가 프록시 서버에 지시할 수 있다.

지시를 포함한 요청 헤더가 들어갈 곳으로서 HTTP/1.0 부터 `Pragma 헤더` 가 정의되어 있다.

`Pragma` 헤더에 포함할 수 있는 페이로드로 유일하게 HTTP 사양으로 정의돈 것이 `no-cache`다.

`no-cache`는 `요청한 콘텐츠가 이미 저장돼 있어도, 원래 서버(오리진 서버)에서 가져오라` 고 프록시 서버에 지시하는 것

`no-cache` 는 HTTP/1.1에 이르러 `Cache-Control` 로 통합됐지만 1.1 이후에도 하위 호환성 유지를 위해 남아있다.

캐시 매커니즘에는 `Pragma: no-cache` 처럼 클라이언트에서 지시하는 것이나 프록시에 대해 지시하는 것도 몇 가지 있지만 그다지 적극적으로 사용되지 않음

프록시가 어느 정도 지시를 이해하고 기대한 대로 동작할지 보증할 수도 없음

중간에서 프록시 하나라도 `no-cache` 를 무시한다면 기대한 대로 동작하지 않는다.

HTTP/2가 등장한 이후로는 보안 접속 비율이 증가했다.

보안 통신에는 프록시가 통신 내용을 감시할 수 없고 중계만 할 수 있다.

프록시의 캐시를 외부에서 적극적으로 관리하는 의미가 이제 없다고도 말할 수 있다.

### ETag 추가

동적으로 바뀌는 요소가 늘어날수록 날짜를 근거로 캐시의 유효성을 판단하기 어려워진다.

그럴 때 사용할 수 있는것이 **RFC 2068**의 HTTP/1.1에서 추가된 `ETag(entity tag)` 다.

`ETag`는 순차적인 갱신 일시가 아니라 파일의 해시 값으로 비교한다.

일시를 이용해 확인할 때 처음처럼 서버는 응답에 `ETag` 헤더를 부여 

두 번째 이후 다운로드 시 클라이언트는 `If-None-Match` 헤더에 다운로드된 캐시에 들어 있던 `ETag` 값을 추가해 요청한다.

서버는 보내려는 파일의 `ETag` 와 비교해서 같으면 **304 Not Modified**로 응답한다.

여기까지는 HTTP/1.0에도 있었던 캐시 제어 구조이다.

### Cache-Control(1)

`ETag` 와 같은 시기에 HTTP/1.1에서 추가된 것이 `Cache-Control` 헤더다

서버는 `Cache-Control` 헤더로 더 유연한 캐시 제어를 지시할 수 있다.

`Expires` 보다 우선해서 처리된다.

먼저 서버가 응답으로 보내는 헤더를 소개한다.

- public: 같은 컴퓨터를 사용하는 복수의 사용자간 캐시 재사용을 허가한다.
- private: 같은 컴퓨터를 사용하는 다른 사용자 간 캐시를 재사용하지 않는다. 같은 URL에서 사용자마다 다른 콘텐츠가 돌아오는 경우에 이용한다
- max-age=n: 캐시의 신선도를 초단위로 설정. 86400을 지정하면 하루동안 캐시가 유효하고 서버에 문의하지 않고 캐시를 이용한다. 그 이후는 서버에 문의한 뒤 **304 Not Modified**가 반환됐을 때만 캐시를 이용한다.
- s-maxage=n: max-age 와같으나 공유 캐시에 대한 설정값
- no-cache: 캐시가 유효한지 매번 문의한다. max-age=0과 거의 같다.
- no-store: 캐시하지 않는다.

`no-cache`는 `Pragma`: `no-cache` 와 똑같이 캐시하지 않는 것은 아니고, 시간을 보고 서버에 접속하지 않은 채 콘텐츠를 재이용하는 것을 그만둘 뿐이다.

갱신 일자와 `ETag`를 사용하며, 서버가 304를 반환했을 때 이용하는 캐시는 유효하다. 캐시하지 않은 것은 `no-cache` 다.

캐시와 개인 정보 보호 관계도 주의해야한다.

`Cache-Control` 은 리로드를 억제하는 시스템이고, 개인 정보 보호 목적으로 사용할 수 없다.

`private` 는 같은 URL이 유저마다 다른 결과를 줄 경우에 이상한 결과가 되지 않도록 지시하는 것이다.

보안 접속이 아니면 통신 경로에서 내용이 보인다.

`no-store` 도 캐시 서버가 저장하지 않을 뿐 캐시 서버가 통신 내용 감시를 억제하는 기능은 없다.

콤마로 구분해 복수 지정이 가능하지만, 내용면에서 다음과 같이 조합한다.

- private, pulic 중 하나. 혹은 설정하지 않는다(기본은 private).
- max-age, s-maxage, no-cache, no-store 중 하나

### Cache-Control(2)

이미 `Pragma: no-cache` 부분에서 `별로 사용할 일이 없다` 라고 설명한 프록시에 대한 캐시 관련 요청이지만, `Cache-Control` 헤더를 요청 헤더에 포함함으로써 프록시에 지시할 수 있다.

서버에서 프록시로 보내는 응답 헤더에 사용할 수 있는 지시도 있다.

우선 클라이언트 측에서 요청 헤더에서 사용할 수 있는 설정값을 소개한다.

- no-cache: Pragma: `no-cache` 와 같다.
- no-store: 응답의 `no-cache` 와 같고, 프록시 서버에 캐시를 삭제하도록 요청
- max-age: 프록시에서 저장된 캐시가 최초로 저장되고 나서 저장 시간 이상 캐시는 사용하지 않도록 프록시에 요청
- max-stale: 지정한 시간만큼 유지 기간이 지났어도 클라이언트는 지정한 시간 동안은 저장된 캐시를 재사용하라고 프록시에 요청한다. 연장시간은 생략할 수 있고, 그런 경우 영원히 유효하다는 의미가 된다.
- min-fresh: 캐시의 수명이 지정된 시간 이상 남아 있을 때, 캐시를 보내도 좋다고 프록시에 요청한다. 즉 적어도 지정된 시간만큼은 신선해야한다.
- no-transform: 프록시가 콘텐츠를 변형하지 않도록 프록시에 요청
- only-if-cached: 캐시된 경우에만 응답을 반환하고, 캐시된 콘텐츠가 없을 땐 **504 Gateway Timeout** 오류 메시지를 반환하도록 프록시에 요청. 이 헤더가 설정되면 처음을 제외하고 오리진 서버에 전혀 엑세스 하지 않는다.

응답 헤더에서 서버가 프록시에 보내는 캐시 컨트롤 지시에는 다음과 같은 것이 있다.

- no-transform: 프록시가 콘텐츠를 변경하는 것을 제어한다.
- must-revalidate: `no-cache` 와 비슷하지만 프록시 서버에 보내는 지시가 된다. 프록시 서버가 서버에 문의했을 떄 서버의 응답이 없으면, 프록시 서버가 클라이언트에 **504 Gateway Timeou**t 이 반환되기를 기대한다.
- proxy-revalidate: `must-revalidate` 와 같지만, 공유 캐시에만 요청한다.

### Vary

`ETag` 설명에서 같은 URL이라도 개인마다 결과가 달라지는 경우를 소개했다.

같은 URL이라도 클라이언트에 따라 반환 결과가 다름을 나타내는 헤더가 **Vary**다.

표시가 바뀌는 이유에 해당하는 헤더명을 `Vary`에 나열함으로써 잘못된 콘텐츠의 캐시로 사용되지 않게 한다.

`Vary: User-Agent, Aceept-Language`

`Vary` 헤더는 검색 엔진용으로도 사용된다.

브라우저 종류에 따라 콘텐츠가 바뀔 수 있다는 것은 모바일 버전은 다르게 보일 수도 있다고 판단할 수 있는 재료가 된다.

그리고 영어 버전, 한국어 버전 등 언어별로 바르게 인덱스를 만드는 힌트도 된다.

모바일 브라우저인지 판정하는 방법은 주로 두 가지다.

1. `User-Agent` 서버에 유저 에이전트 정보를 보내면 서버에서는 이 정보를 바탕으로 콘텐츠를 나눠 내보낼 수 있다.

## 리퍼러

리퍼러(헤더명: `Refere`) 는 사용자가 어느 경로로 웹사이트에 도달했는지 서버가 파악할 수 있도록 클라이언트가 서버에 보내는 헤더

웹 페이지가 이미지나 스크립트를 가져올 경우는 리소스를 요청할 때 리소스를 이용하는  HTML 파일의 URL이 리퍼러로서 전송된다.

	Referer: http://www.example.com/link.html

만약 북마크에서 선택하거나 주소창에서 키보드로 직접 입력했을 때는 `Referer` 태그를 전송하지 않거나 `Referer:abount:blank` 를 전송한다.

예를 들어 검색 엔진은 검색 결과를 `?q=키워드` 형식의 URL로 표시한다.

브라우저가 이 URL을 리퍼러로서 전송하면, 서버는 어떤 검색 키워드로 웹사이트에 도달했는지 알 수 있다.

웹 서비스는 리퍼러 정보를 수집함으로써 어떤 페이지가 자신의 서버에 링크를 걸었는지도 알 수 있다.

**GET 파라미터**는 리퍼러를 통해 외부 서비스로 전송되어 **개인 정보 유출**로 이어지기 때문에 웹 서비스 설계자는 개인 정보가 **GET 파라미터**로 표시되게 만들어선 **안된**다.

방어적인 용도로 이미지에 직접 링크되는 것을 막고자 이미지 다운로드 시 리퍼러가 설정되지 않은 요청은 거절하는 경우도 있다.

폼이 설정되지 않은 외부 사이트에서 요청을 보내는 공격(CSRF)을 막을 목적으로 사용하기도 했지만, 브라우저에서 전송하지 않도록 설정해버리면 정상적으로 동작하지 않아 현재는 이런 식으로 보안 대책을 세우는 경우는 없다.

리퍼러 정책으로서 설정할 수 있는 값에는 다음과 같은 것이 있다.

- no-referrer: 전혀 보내지 않는다.
- no-referrer-when-downgrade: 현재 기본 동작과 마찬가지로 HTTPS→HTTP 일 때는 전송하지 않는다.
- same-origin: 동일 도메인 내의 링크에 대해서만 리퍼러를 전송
- origin: 상세 페이지가 아니라 톱페이지에서 링크된 것으로 해 도메인 이름만 전송
- strict-origin: origin과 같지만 HTTPS→HTTP 일 때는 전송하지 않는다.
- origin-when-crossorigin: 같은 도메인 내에서는 완전 리퍼러를, 다른 도메인에서는 도메인 이름만 전송
- strict-origin-when-crossorigin: origin-when-crossorigin과 같지만 HTTPS→HTTP 일 때는 송신하지 않는다.
- unsafe-url: 항상 전송

이 밖에도 `Content-Security-Policy` 헤더로 지정할 수 있다. `Content-Security-Policy` 헤더는 많은 보안 설정을 한꺼번에 변경할 수 있는 헤더다.

`Content-Security-Policy: referrer origin` 

## 검색 엔진용 콘텐츠 접근 제어

자동 순회 프로그램은 `크롤러`, `로봇`, `봇` , `스파이더` 같은 이름으로 불린다.

이 크롤러의 접근을 제어하는 방법은 주로 두가지가 널리 사용된다.

- robots.txt
- 사이트맵

### robots.txt

`robots.txt` 는 서버 콘텐츠 제공자가 크롤러에 접근 허가 여부를 전하기 위한 프로토콜이다.

규칙을 기술한 파일명이다.

이 규칙을 해석해 실제로 접근을 중간하는 것은 크롤러 쪽이므로, 크롤러 개발자들의 신사협정이라 할 수 있다.

`robots.txt`는 다음과 같은 형식으로 읽기를 금지할 크롤러의 이름과 장소를 지정

    User-agent: *
    Disallow: /cgi-bin/
    Disallow: /tmp/

위에서는  `/cgi-bin/` 폴더와 `/tmp/`  폴더 접근을 금지했다.

`User-agent` 에 구글 봇처럼 개별적으로 지정할 수 있다.

비슷한 내용을 HTML의 메타 태그로도 기술할 수 있다. `robots.txt` 가 우선하지만, 메타 태그로 더 자세히 지정할 수 있다.

`<meta name="robots" content="noindex"` content 속성에는 다양한 디렉티브를 기술 할 수 있다.

같은 디렉터리브는 HTTP의 `X-Robots-Tag` 헤더에도 쓸 수 있다.

	X-Robots-Tag: noindex, noffolow

### 사이트맵

웹 사이트에 포함된 페이지 목록과 메타데이터를 제공하는 XML 파일

`robots.txt` 가 블랙리스트처럼 사용된다면, 사이트맵은 화이트리스트처럼 사용된다.

크롤러는 링크를 따라가면서 페이지를 찾아내는데, 플래시로 만든 콘텐츠나 자바스크립트를 잔뜩 써서 만들어진 동적 페이지의 링크처럼 크롤러가 페이지를 찾을 수 없는 경우라도 사이트맵으로 보완할 수 있다.

구글의 경우는 사이트맵을 사용해 웹사이트의 메타데이터를 검색 엔진에 전달할 수 있다.

- 웹사이트에 포함되는 이미지의 경로, 설명, 라이선스, 물리적인 위치
- 웹사이트에 포함되는 비디오 섬네일, 타이틀, 재생 시간, 연령 적합성 등급이나 재생 수
- 웹사이트에 포함되는 뉴스의 타이틀, 공개일, 카테고리, 뉴스에서 다루는 기업의 증권코드

---


# 3장. GO 언어를 이용한 HTTP/1.0 클라이언트 구현이 장에서는 Go 언어로 위의 내용을 송수신하는 코드 작성법을 학습한다.

## Go 언어를 사용하는 이유

- 다른 언어보다 간결한 언어 사양과 풍부한 표준 라이브러리를 갖추고 있다.
- 컴파일이 동적 스크립트 언어를 실행하는 만큼 빠르고 형 검사가 확실히 된다.
- 실행 속도도 빠르고 멀티 코어의 성능을 끌어내기 쉬우며 메모리를 절약한다.
- 크로스 컴파일이 간단하다.
- 아웃풋이 단일 바이너리로 되므로 배포하기 쉽다.
- 언어 사양이 다른 언어보다 작아, 다른 언어 사용자가 보더라도 동작을 이해하기 쉬우므로 의사(peeudo) 언어로써 뛰어나다.
- 컴파일 언어라서 구문과 형 검사가 이루어지므로 입력 실수를 개닫기 쉽다.
- 표준 라이브러리만으로 HTTP 액세스를 하는 프로그램을 만들 수 있다.
- 실제로 다양한 웹 서비스의 CLI 클라이언트 구현 언어로서 사용된다.

**Go 언어의 API 구성**

- 기능이 제한적이지만, 간편하게 다룰 수 있는 API
- 쿠키도 이용할 수 있고 약간 컨트롤 가능한 API
- 모든 기능에 액세스할 수 있는 프리미티브 API

---

# 4장. HTTP/1.1의 신택스: 고속화와 안전성을 추구한 확장

이 장에서 다룰 프로토콜 신택스로서 HTTP/1.1 의 변경 사항은 다음과 같다

**통신 고속화**

- Keep—Alive 가 기본적으로 유효하다.
- 파이프라이닝

**TLS에 의한 암호화 통신을 지원**

**새 메서드 추가**

- PUT과 DELETE가 필수 메서드가 됐다.
- OPTION, TRACE, CONNECT 메서드가 추가됐다.

**프로토콜 업그레이드**

**이름을 사용한 가상 호스트를 지원**

**크기를 사전에 알 수 없는 콘텐츠의 청크 전송 인코딩 지원**

## 통신 고속화

HTTP/1.1 시대에 규격화된 것 가운데 가장 큰 주제는 TLS을 통한 안전한 통신 제공이다.

이 기능은 인증서 등 각종 설정이 필요하다.

그 밖에 HTTP/1.1만 사용했을 때 얻을 수 있는 혜택으로는 통신 고속화를 들 수 있다.

캐시는 콘텐츠 리소스마다 통신을 최적화하는 기술이지만, 이 장에서 소개하는 Keep-Alive와 파이프 라이닝은 좀 더 범용적으로 모든 HTTP 통신을 고속화하는 기능이다.

### Keep-Alive

Keep-Alive 는 HTTP의 아래층인 TCP/IP 통신을 효율화하는 구조다.

Keep-Alive를 사용하지 않으면 하나의 요청마다 통신을 닫아야 하지만, Keep-Alive를 사용하면 연속된 요청에는 접속을 다시 이용한다.

이로써 TCP/IP는 접속까지의 대기 시간이 줄어들고, 통신 처리량이 많아지므로 속도가 올라간 것처럼 느껴진다.

모바일 통신에서는 배터리 낭비도 줄어든다.

HTTP/1.0 에서는 요청 헤더에 다음 헤더를 추가함으로써 Keep-Alive를 이용할 수 있다.

`Connection: Keep-Alive`

HTTP 아래 계층의 프로토콜인 TCP/IP 도 접속할 때는 1.5회 왕복의 통신을 필요로 한다.

패킷이 1회 왕복하는 시간을 1RTT(round-trip-time)로 부르며, TLS에서는 서버/클라이언트가 통신을 시작하기 전에 정보를 교환하는 핸드세이크(handshake) 과정에서 2RTT만큼 시간이 걸린다.

이때 Keep-Alive를 이용하면, 핸드셰이크 횟수를 줄일 수 있다.

여러 번 반복되는 핸드셰이크를 줄임으로써 응답 시간을 개선할 수 있다.

Keep-Alive를 이용한 통신은 클라이언트나 서버 중 한 쪽이 다음 헤더를 부여해 접속을 끊거나 타임아웃 될 때까지 연결이 유지된다.

`Connection: Close` 

통신 종료가 규정되어 있긴 하지만, 모든 통신이 확실히 끝났는지를 서버가 판정할 수 없다.

자바스크립트를 이용하면 동적으로 요청을 발신할 수도 있으므로, HTML을 정적으로 해석하는 것만으로는 클라이언트 측에서 모든 통신의 완료를 탐지할 수 없다.

그 때문에 서버에서 Keep-Alive 종료를 명시적으로 보내는 것이 간단하지 않고, 실제로는 타임아웃으로 접속이 끊어지기를 기다리게 된다.

Keep-Alive 지속 시간은 클라이언트와 서버 모두 가지고 있다.

한쪽이 TCP/IP 연결을 끊는 순간에 통신은 완료되므로, 어느 쪽이든 짧은 쪽이 사용된다.

통신이 지속되는 동안 OS의 자원을 계속 소비하므로, 실제로 통신이 전혀 이루어지지 않는데 접속을 유지하는 것은 바람직하지 않다.

짧은 시간에 접속을 끊는 것에 의미가 있다.

### 파이프라이닝

파이프라이닝도 고속화를 위한 기능이다.

파이프라이닝은 최초의 요청이 완료되기 전에 다음 요청을 보내는 기술이다.

다음 요청까지의 대기 시간을 없앰으로써, 네트워크 가동률을 높이고 성능을 향상시킨다.

Keep-Alive 이용을 전제로 하며, 서버는 요청이 들어온 순서대로 응답을 반환한다.

왕복시간이 거리는 모바일 통신에서 큰 효과를 기대할 수 있다.

현재 기본으로 활성화한 브러우저는 오페라와 IOS 5이후의 사파리정도다.

요청받은 순서대로 응답해야하므로, 응답 생성에 시간이 걸리거나 크기가 큰 파일을 반환하는 처리가 있으면 다른 응답에 영향을 준다. 이는 HOL 블로킹(head-of-line-blocking)이라 불리는 문제다.

### 전송 계층 보안( TLS)

HTTP/1.1과 병행해 통신 경로를 암호화하는 전송 계층 보안(Transport Layer Security TLS) 이 규격화됐다.

TLS 암호화 자체는 HTTP뿐만 아니라 다양한 형식의 데이터를 양방향으로 흘려보낼 수 있다.

TLS는 기존 프로토콜에 통신 경로의 안전성을 추가해 새로운 프로토콜을 만들어 낼 수 있는 범용적인 구조로 되어있다.

HTTPS는 443번을 사용해 다른 서비스로 취급된다.

HTTPS 이외에 메일 전송 프로토콜 SMTP(25번 포트)의 TLS 버전인 SMPTS(465번) 등 기존 프로토콜의 버전업에도 이용된다.

HTTP 통신을 중계하는 게이트웨이 입장에서 보면,  `암호화되어 통신 내용을 엿보거나 변경할 수 없는 양방향 통신` 이다.

HTTP/1.0과 1.1에서는 프록시 서버 등이 통신을 해석해 캐시함으로써 고속화 기능을 제공할 수 있었지만, 자신이 해석할 수 없는 프로토콜을 멈춰버리는 경우가 있었다.

TLS를 사용하면 조작할 수 없는 안정된 통신로가 생기므로, HTML5에서 새로 도입된 웹소켓 같은 통신 프로토콜이나 HTTP/2등 HTTP/1.1 이전과 상위 호환성이 없는 수많은 새로운 시스템을 원만하게 도입하는 인프라가 됐다.

### 해시 함수

이볅 데이터를 규칙에 따라 집약해감으로써 해시 값으로 불리는 짧은 데이터를 만들어 낸다.

해시 함수에는 암호화 통신을 하는 데 편리한 수학적 특성이 있다.

- 같은 알고리즘과 같은 입력 데이터라면, 결과로서 생성되는 값은 같다. h(A)=X 가 항상 성립한다.
- 해시 값은 알고리즘이 같으면 길이가 고정된다. SHA-256 알고리즘에선 256비트(32바이트)다. 따라서 입력 데이터가 너무 작을 경우 해시 값이 더 커지지만, 기본적으로는 len(X)<len(A)가 된다.
- 해시 값에서 원래 데이터 유추하기 어렵다. h(A)=X의 X에서 A를 찾기 곤란하다(약한 충돌 내성).
- 같은 해시 값을 생성하는 다른 두 개의 데이터를 찾기 어렵다. h(A)=h(B)가 되는 임의의 데이터 A, B를 찾기가 곤란하다(강한 충돌 내성).

### 공통 키 암호와 공개 키 암호 그리고 디지털 서명

암호화에서 중요한 것은 알고리즘이 알려져도 안전하게 통신할 수 있는 것이다.

인터넷에서는 다양한 운영체제가 설치된 컴퓨터와 휴대 단말의 브라우저 간에 안전하게 통신할 수 있음을 보증해야 한다.

현재 일반적으로 사용하는 방식은 암호화 알고리즘은 공개하고, 그 암호화에 사용하는 데이터(키)를 따로 준비하는 방식이다.

TLS에서 사용되는 방식으로는 공통  키 방식과 공개 키 방식 두 종류가 있다.

공통 키 방식(대칭암호)

마치 무인 택배함과 같아서 택배함을 잠글 때 설정한 것과 같은 번호로 연다.

따라서 통신하는 사람끼리는 이 번호를 공유할 필요가 있다.

암호화는 키 데이터로 원복데이터를 파괴하는것이지만 키만 있으면 망가진 데이터를 원래대로 수리할 수 있으므로, 받은 쪽에서 데이터를 복원해 읽는다.

TLS에서는 일반 통신의 암호화에 사용한다.

공개 키 방식(비 대칭암호)

공개 키 방식에서 필요한 것은 공개 키와 비밀 키다.

공개 키는 공개해도 문제없지만, 비밀 키는 알려져서는 안된다.

암호화 하는 것이 공개 키고 해독하는 것은 비밀 키다.

디지털 서명은 공개 키 방식을 응용한 예다.

거꾸로 열쇠를 나누어주고 자물쇠를 비밀로 해두는 것과 같은 이미지다.

실제 디지털 서명은 본문 자체를 암호화하는 것이 아니라 먼저 해시화하고 그 결과를 암호화한다.

이런 용도에서는 자물쇠 쪽을 비밀로 하므로 자물쇠 쪽이 비밀 키가 되고 열쇠 쪽이 공개 키가 된다.

### 키 교환

키 교환은 클라이언트와 서버 사이에 키를 교환하는 것이다.

간단한 방법으로는 클라이언트에서 공통 키를 생성한 다음 전술한 서버 인증서의 공개 키로 암호화해 보내는 방법이 있고,

키 교환 전용 알고리즘도 있다.

### 공통 키 방식과 공개 키 방식을 구분해서 사용하는 이유

공통 키 방식과 공개 키 방식을 비교하면, 공개 키 방식이 복잡한 만큼 아무래도 안정성이 높다.

하지만 TLS는 이 양쪽 방식을 조합했다.

TLS에서는 통신마다 한 번만 사용되는 공통 키를 만들어 내고, 공개 키 방식을 사용해 통신 상대에서 신중히 키를 전달한 이후는 공통 키로 고속으로 암호화 하는 2단계 방식을 이용한다.

공개 키 방식이 안전성이 높지만, 키를 가지고 있어도 암호화와 복호화에 필요한 계산량이 공통 키 방식보다 너무 많기 때문

### TLS 통신 절차

TLS 통신으 크게 셋으로 나눌 수 있따.

처음이 핸드셰이크 프로토콜로 통신을 확립하는 단계,

다음이 레코드 프로토콜로 불리는 통신 단계,

마지막이 SessionTicket 구조를 이용한 재접속 시의 고속 핸드셰이크다.

각 단계를 소개할 때 핸드셰이크에는 크게 두 개의 과제가 있으므로 나누어 소개한다.

**서버의 신뢰성 확인**

서버의 신뢰성을 보증하는 구조는 공개 키를 보증하는 구조이기도 해서, 공개 키 기반구조(public key infrastucture(PKI)라고 불린다.

브라우저는 서버에서 그 서버의 SSL 서버 인증서를 가져오는 것부터 시작한다.

### 키 교환과 통신 시작

공개 키 암호를 사용하는 방법과 키 교환 전용 알고리즘을 사용하는 방법이 있다.

클라이언트는 먼저 난수를 사용해 통신용 공통 키를 만든다.

공개 키를 사용하는 방법은 간단하다.

서버 인증서에 첨부된 공개 키로 통신용 공통 키를 암호화해 그 키를 서버에 보낸다.

서버는 인증서의 공개 키에 대응하는 비밀 키를 갖고 있으므로 건네받은 데이터를 복호화해 공통 키를 꺼낼 수 있다.

키 교환 전용 알고리즘을 사용할 때는 키를 생성할 시드를 클라이언트와 서버 양쪽에서 하나씩 만든다.

만들어진 시드를 서로 교환해서 계산한 결과가 공통 키가 된다.

시드를 서로 교활할 때 공개 키 암호화가 함께 사용된다.

### 통신

통신을 할 때도 기밀성과 무결성(조작 방지)을 위해 암호화 한다.

암호화에는 공통 키 암호화 방식 알고리즘을 이용

### 통신의 고속화

통신에서 전기 신호가 서버에 도달하고 응답이 되돌아오기까지의 시간은 매우 긴 시간이다.

그래서 인터넷을 더 빠르게 하려면 왕복 시간을 줄이는 것이 중요하다.

TLS와 HTTP에는 이를 위한 장치가 몇 가지 구현되어 있다.

이 장 처음에 소개한 `Keep-Alive`다. 

`Keep-Alive` 를 이용하면 세션이 지속되므로, 최초 요청 이후의 통신에는 RTT가 1이된다.

TLS 아래 계층을 핸드세이크가 필요한 세션형 TCP에서 재전송도 및 흐름 제어도 하지 않는 간이 데이터그램형 UDP로 대체해, 애플리케이션 계층에서 재전송하는 QUIC(quick UDP internet connections)라는 통신 방식으 RFC화 IETF에 제안됐다.

HTTP나 TLS 통신 이전에 전송 계층인 TCP 시점에서 핸드셰이크에 1RTT를 소비했지만, UDP는 핸드셰이크를 하지 않으므로 0RTT로 연결할 수 있다.

인텔이 TLS 통신에 사용되는 AES 고속화 명령을 CPU에 추가했다.

전용 명령으로 가속이 이루어져, HTTP/2에서는 서버와 클라이언트 간 세션 수가 최대 1/6로 줄어들었다.

핸드셰이크 횟수도 줄었다.

TLS/1.3과 QUIC을 이용하면 핸드셰이크 비용도 내려가므로 1회당 통신 부하는 더욱 줄어든다.

### 프로토콜 선택

TLS가 제공하는 기능 중 차세대 통신에 없어선 안 될 것이 애플리케이션 계층 프로토콜을 선택하는 확장 기능이다.

ALPN에서는 TLS의 최초 핸드셰이크 시(ClientHello) 클라이언트가 서버에 `클라이언트가 이용할 수 있는 프로토콜 목록` 을 첨부해서 보낸다.

서버는 그에 대한 응답(ServerHello)으로 키 교환을 하고 인증서와 함께 선택한 프로토콜을 보낸다.

클라이언트가 보낸 목록에서 서버가 사용할 프로토콜을 하나 골라 반환하는 방법은 2장에서 소개한 콘텐트 니고시에이션과 같다.

선택할 수 있는 프로토콜 목록은 IANA에서 관리한다.

주로 HTTP 계열과 WebRTC 계열 프로토콜이 있다.

이 가운데 중요한 것이 HTTP/1.1과 HTTP/2 및 HTTP/2의 전신이 된 SPDY의 각 버전이 공존한다는 것이다.

TLS를 사용하면 중간에 프록시의 간섭을 받지 않고, 프로토콜 버전을 사전에 서버와 조정함으로써 전혀 호화성이 없는 프로토콜도 이용할 수 있다.

### TLS가 지키는 것

TLS는 통신 경로의 안전을 지키기 위한 구조이고, 클라이언트와 서버 간 통신 경로를 전혀 신뢰할 수 없는 상태에서도 안전하게 통신할 수 있도록 설계됐다.

통신 경로를 신뢰할 수 없다는 것은 중간자가 통신을 감청하거나 통신 내용을 자유롭게 변경하거나 클라이언트로 속여 요청을 보낼 수 있다는 뜻이지만, TLS는 그 상태에서도 도청도 조작도 사칭도 할 수 없는 안전한 통신을 제공한다.

TLS는 통신 경로 밖의 정보는 숨겨주지 않는다.

브라우저의 쿠키를 뺴내는 크래킹은 TLS로 보호하고 있어도, 브라우저를 오동작시켜 의도치 않은 서버로 보낼 수 있다.

그리고 서버가 크랙됐을 때도 정보가 보호되지 않는다.

사용자의 패스워드를 평문으로 데이터베이스에 저장하지 않고, 해시화해서 보호하는 것은 TLS와 관계없이 꼭 해야하는 일이다.

### OPTIONS

`OPTIONS` 메서드는 서버가 받아들일 수 있는 메서드 목록을 반환한다.

응답 중에서 `Allo` 헤더에 결과가 들어있다.

그러나 대부문 웹 서버는 `OPTIONS` 메서드를 허용하지 않는다.

점유율이 높은 nginx도 기본적으로 다룰 수 없고, `405 Method Not Allowed` 를 반환한다.

`OPTIONS` 메서드는 브라우저가 다른 서버에 요청을 보낼 때, 사전 확인에 사용되는 경우가 있다.

### TRACE(TRACK)

서버는 `TRACE` 메서드를 받으면 `Content-Type`에 `messasge/http` 를 설정하고, 스테이터스 코드 200 ok를 붙여 요청 헤더와 바디를 그대로 반환한다.

그러나 현재는 거의 사용되지 않는다.

임의의 스크립트를 브라우저에서 실행하는 크로스 사이트 스크립팅 (cross-site-scripting XSS)의 취약성과 조합해, BASIC 인증 사용자 이름과 패스워드를 장악할수 있는 크로스 사이트 트레이싱(cross-site tracing XST) 이라는 취약성이 유명해져 웹상에서 나오는 정보도 TRACE 메서드를 무효화하는 설정법 뿐이다.

### CONNECT

`CONNECT` 는 HTTP 프로토콜상에 다른 프로토콜의 패킷을 흘릴 수 있게 한다.

프록시 서버를 거쳐, 대상 서버에 접속하는 것을 목적으로 한다.

주로 https 통신을 중계하는 용도로 사용

`CONNECT` 메서드를 무조건 받아들이는 프록시는 아무 프로토콜이나 통과시켜버리므로, 맬웨어가 메일을 보내거나 하는 통신 경로로 사용될 위험이 있다.

## 프로토콜 업그레이드

HTTP/1.1 부터는 HTTP 이외의 프로토콜로 업그레이드 할 수 있게 됐다.

HTTP/1.0과 HTTP/1.1은 텍스트 기반의 알기 쉬운 프로토콜이지만, 이 기능을 사용해 이진 프로토콜로 교체할 수 있다.

업그레이드는 클라이언트 측에서 요청 할 수도 있고 서버 측에서 요청할 수 있다.

다음 세 종류의 업그레이드가 있다

- HTTP에서 TLOBOBOBOBOBOBOBOBOBOBOBOBOBOBS를 사용한 안전한 통신으로 업그레이드(TLS/1.0, TLS/1.1, TLS/1.2)
- HTTP에서 웹소켓을 사용한 양방향 통신으로 업그레이드(websocket)
- HTTP에서 HTTP/2로 업그레이드(h2c)

현재는 모든 통신이 TLS화 되고 있으며, TLS 자체가 갖는 핸드셰이크 시 프로토콜 선택 기능(ALPN)을 사용하도록 권장하고 있다.

HTTP/2 에서는 프로토콜 업그레이드 기능이 삭제됐다.

HTTP/2 통신도 TLS를 전제로 하고, TLS의 ALPN 사용을 권장한다.

현재 프로토콜 업그레이드는 거의 웹소켓용이다.

### 클라이언트 쪽에서 업그레이드를 요청

클라이언트 쪽에서  업그레이드를 하는 경우 우선 Upgrade와 Connection 헤더를 포함한 요청을 보낸다.

만약 업그레이드를 지원하지 않는 HTTP/1.0 서버가 암호화되지 않은 상태로 GET 요청을 반환해버리면, 비밀로 해야 할 애용이 노출되는 문제가 있다.

그런 경우 클라이언트는 우선 OPTTIONS 요청을 보내 업그레이드할 수 있는지 정보를 가져온다.

### 서버 쪽에서 업그레이드를 요청

서버 쪽에서  TLS로 갱신을 요청하는 경우 스테이터스 코드 426을 붙인다.

단 이 경우도 즉석에서 핸드셰이크를 하는 것은 아니고 클라이언트 쪽에서 다시 프로토콜 전환을 요청한 후에 핸드셰이크가 이루어진다.

### TLS 업그레이드의 문제점

TLS 업그레이드는 프록시가 악의를 가지고 정보를 훔치거나 의도와 다른 요청을 서버에 보내는 중간자 공격에 약하다는 단점이 있다.

클라이언트가 업그레이드 요청을 보내 클라이언트/프록시 사이가 TLS화 되더라도 그 너무는 암호화되지 않은 상태일지도 모르고, 프록시가 정보를 읽을 수 있게 된다는 문제가 있다.

TLS를 사용하려면 클라이언트와 서버 간의 통신 경로 전체를 암호화 하지 않으면 의미가 없다.

이외에도 TLS 통신으로의 업그레이드 방법이 있다.

우선은 리디렉트 기능으로 `https://`로 시작되는 페이지로 유도해서 실행할 수 있다.

구글의 가이드라인에서도 `301` 을 사용한 리디렉션을 권장한다.

그 밖의 방법으로는 **RFC 6797**로 정의된 HTTP Strict Transport Security(HSTS)가 있다.

## 청크

HTTP/1.1에서 지원되는 새로운 데이터 표현으로. 전체를 한꺼번에 전송하지 않고 작게 나눠 전송하는 **청크**(chunk)방식이 있다.

클라이언트 측의 장점으로는 서버 측에서 마지막 데이터 준비가 됐을 무렵엔 그 전까지의 데이터는 이미 전송이 끝났으므로 리드 타임을 짧게 할 수 있다.

### 메세지 끝에 헤더 추가

청크 형식으로 전송하는 경우에 청크된 메시지 끝에 헤더를 추가할 수 있게 됐다.

`Trailer: Content-Type`

`여기서 부여한 헤더는 바디를 보낸 후 전송된다` 라고 알려준다.

청크 형식으로만 사용할 수 있다는 것은 청크 형식임을 사전에 알 수 있게 해야한다.

이를 위해 필요한 헤더는 지정할 수 없다.

또한 `Trailer` 자신을 나중에 보낼 수 없다.

따라서 다음의 헤더는 지정할 수 없다.

- Transfer_Encoding
- Content-Length
- Trailer

## 바디 전송 확인

클라이언트에서 서버로 한 번에 데이터를 보내는게 아니라, 일단 받아들일 수 있는지 물어보고 나서 데이터를 보내는 2단계 전송을 할 수 있게 됐다.

우선 클라이언트는 다음 헤더와 바디를 제외한 모든 헤더를 지정해 문의한다.

파일이 없어도 `Content-Length` 헤더를 함께 보낸다

`Expect: 100-continue` 

만약 서버로부터 다음과 같은 응답이 돌아왔다면, 서버가 처리할 수 있다는 말이므로 바디를 붙여 다시 전송한다

`100 Continue`

서버가 지원하지 않으면 **417 Exceptation Failed** 가 돌아오기도 한다.

---

# 5장. HTTP/1.1의 시맨틱스: 확장되는 HTTP의 용도

이 장에서는 HTTP/1.1 이후에 확장된 프로토콜과 새로운 규약을 사용한 다양한 사례를 소개한다.

## 파일 다운로드 후 로컬에 저장

브라우저가 파일을 어떻게 처리할지 결정하는 것은 확장자가 아니라 서버가 보낸 MIME타입이다.

예를 들면 서버의 응답에 `Content-Disposition` 헤더가 있으면 브라우저는 다운로드 대화상자를 표시하고 파일을 저장한다.

브라우저가 페이지를 표시할 때 `content-Disposition` 헤더가 있으면, 페이지 표시를 리셋하지 않고 다운로드만 한다.

그 점을 이용해서 다운로드 완료 페이지를 보여줄 수 있다.

## 다운로드 중단과 재시작

다운로드 시간이 오래 걸리면, 통신이 불안정해지거나 다운로드 도중에 실패할 확률이 높아진다.

중간부터 재시작 한다는 것은 큰 파일에서 지정한 범위를 잘라내 다운로드한다는 말이다.

서버가 범위 지정 다운로드를 지원하는 경우는 `Accept-Ranges` 헤더를 응답에 부여한다.

`Accept-Ranges: bytes` 

`Accept-Ranges` 헤더는 두 가지 값을 가질 수 있다.

- Accept-Ranges: bytes: 범위 지정 다운로드를 받아들인다. 단위는 바이트.
- Accept-Ranges: none: 범위 지정 다운로드를 받아들이지 않는다.

주의할 것은 재다운로드 시 대상 파일이 중간에 변경되면, 이미 다운로드해둔 파일 조각의 가치가 없어지므로 파일 변경을 탐지할 필요가 있다는 점이다.

### 복수 범위 다운로드

`Ranges`헤더로 복수의 범위를 지정할 수도 있다.

그런 경우 멀티파트 폼과 비슷한 `multipart/byteranges` 라는 Content-Type으로 결과가 돌아온다.

`Range: bytes=500-999, 7000-7999`

멀티파트 폼은 요청에 많은 데이터를 넣으려고 사용했지만, multipart/byteranges는 응답에 많은 데이터 조각을 넣으려고 사용한다.

### 병렬 다운로드

다운로더에 잘 구현되는 기능으로 병렬 다운로드가 있따.

서버가 세션마다 대역을 제한할 경우, 영역을 나눠 세션마다 `Range` 헤더를 이용해 HTTP 접속을 하는 것으로 병렬로 다운로드할 수 있었다.

다운로드한 데이터 조각을 나중에 결합하면, 전체 다운로드 시간이 줄어든다. 

다만 병렬 다운로드는 서버에 지나치게 부담을 주기 때문에 별로 권장되진 않는다.

브라우저의 경우는 같은 웹사이트에 대한 동시 접속 수를 1~6으로 재한하고 있다.

중간 회선이 병목이 되면 아무리 병렬화해도 속도는 변하지 않고, 통신량이 몰려 오히려 늦어질 수 잇다.

요즘은 정적 파일을 캐시해 서버에 부담을 주지 않고 배포하는 CDN(content delivery network)이 보급돼, 다운로드가 몰려 속도가 떨어지는 일도 줄었다.

OS 설치 이미지 처럼 큰 파일을 빠르게 다운로드해야 할 때는 고속 다운로드를 실현하는 시스템을 이용하자.

## XMLHttpRequest

XMLHttpRequest는 지금까지 소개한 HTTP 통신과 마찬가지로 클라이언트가 서버에 요청을 보내고, 그 응답으로 서버가 클라이언트에 데이터를 보낼 수 있다는 것이다.

헤더를 송수신할 수도 잇고, 캐시 제어나 쿠키 송수신 등 내용은 거의 변함이 없다.

FormData 클래스를 사용하면, multipart/form-data 형식으로 파일 등을 보낼 수도 있다.

단 HTTP처럼 서버 측에서 클라이언트에 요청을 보낼 수는 없다.

### XMLHttpRequest와 브라우저의 HTTP 요청 차이

브라우저에서 HTML을 읽고 폼으로 데이터를 전송하는 처리를 비교하면 다음이 다르다.

- 송수신할 때 HTML 화면이 새로 고침되지 않는다.
- GET과 POST 이와의 메서드도 전송할 수 있다.
- 폼의 경우 키와 값이 일대일이 되는 형식의 데이터만 전송할 수 있고, 응답은 브라우저로 표시되어 버리지만(XML이라는 이름과 달리), 플레인 텍스트, JSON, 바이너리 데이터, XML등 다양한 형식을 송수힌 할 수 있다.
- 몇 가지 보안상 제약이 있다.

XMLHttpRequest 를 사용하면 자바스크립트 내에서 송수신이 완결되므로 화면이 지워지지 않아도 최신 정보를 서버에서 가져올 수 있다.

이처럼 화면을 지우지 않고 웹페이지를 읽어오거나 시간이나 타이밍에 따라 몇 번이고 갱신할 수 있는 아키텍처를 Ajax(asynchronous JavaScript+XML의 약자)라고 부른다.

XMLHttpRequest는 다양한 포맷을 지원한다.

responseType에 문자열을 설정함으로써 반환값을 어떤 오브젝트로서 response에 저장할지 결정할 수 있다.

### 코멧

브라우저의 HTTP 요청도 XMLHttpRequest도 클라이언트에서 서버로 데이터를 보낸다.

실시간 양방향 통신을 하는 기술을 **코멧**(comet)이라 부른다.

단방향 통신을 이용해 양방향 통신을 하기 위해서는 두 가지 방법이 있다.

하나가 폴링, 다른 하나는 롱 폴링이다.

**폴링**

통지를 받는 쪽에서 번번하게 통지가 없는지 물으러 가는 방식

불필요한 요청과 응답이 발생하므로, 송수신 모두 대역과 CPU를 소비한다.

클라이언트가 모바일 단말이라면 소비 전력도 문제가 된다.

**롱 폴링**

클라이언트가 서버에 요청을 보내면, 서버는 바로 응답하지 않고 응답을 보류한 채 대기한다.

HTTP 통신에서는 서버가 통신을 종료하거나 요청이 타임아웃될 떄 까지는 클라리언트로 응답이 돌아오지 않는다.

접속 완료 권한이 서버에 있는 점을 응용해 서버의 응답을 자유로운 타이밍에 돌려줌으로써, 서버의 요청인 것처럼 가장해 정보를 송신한다.

**단점**

우선 HTTP는 서버에서 클라이언트로 메시지를 보내는 전용 API가 아니고, 쿠키 등을 포함한 대량의 헤더를 부여해 송수신하는 구조다.(메시지당 오버헤드는 큰편이다)

일단 서버에서 메시지를 보내면, 클라이언트 쪽에서 세션을 다시 연결하지 않는 이상 통신을 보낼 수 없다.

서버로부터의 연속된 메시지 전송에는 강하지 않다.

### XMLHttpRequest의 보안

보안상 XMLHttpRequest에는 몇 가지 제한이 설정돼 있다.

XMLHttpRequest의 보안 제어는 액세스할 수 있는 정보 제한과 전송 제한이라는 두 가지 제한으로 구성된다.

우선 액세스할 수 있는 정보의 제한으로 **쿠키**가 있다.

스크립트로  `document.cookie` 속성에 엑세스하면 브라우저에서 여는 페이지에 관한 쿠키를 모두 읽을수 있다.

`httpOnly` 속성을 쿠키에 부여하면, 스크립트로 액세스 할 수 없어지므로, 임의의 스크립트가 삽입되더라도 보안상 바람직하지 않은 쿠키를 외부로 유출될 위험이 줄어든다.

전송 제한에는 `도메인`,  `메서드` , `헤더` 세 종류가 있다.

우선 동일-출처 정책(same origin policy)으로 `요청을 보낼 수 있는 도메인 제한`  이 있다.

기본적으로 브라우저가 액세스하고 있는 호스트에만 접근 할 수 있다.

그 박의 사이트에 액세스 하는 방법으로서 XMLHttpRequest뿐만 아니라 브라웆어ㅔ서 널리 이용되는 교차 출처 리소스 공유(cross-origin resource sharing CORS) 라는 액세스 제한 시스템이 있다.

또 하나의 전송 제한은 이용할 수 있는 메서드를 제한하는 것

`CONNECT`, `TRACE`, `TRACK`(IIS용 TRACE의 다른이름)을 지정하면 `open()` 메서드를 호출할 떄 SecurityError 예외를 보낸다

헤더는 현재의 포로토콜 규약이나 환경에 영향을 미치는 것, 쿠키처럼 보안에영향을 주는 것, 브라우저의 능력을 넘을 수 없을 것 등이 금지되어있다.

브라우저의 능력을 넘을 수 없다는 것은 브라우저 자신이 지원하지 않는 압축 형식을 **Accept-Encoding**으로 지정하는 행위를 할 수 없다는 뜻이다.

앞으로 사용될 것에 대비해서, **Sec-**으로 시작되는 키나 **Proxy-**로 시작되는 키도 금지되어 있다.

용도에 따라서는 XMLHttpRequest보다도 쿠키에 엄격한 제한이 걸린 경우가 있다.

다양한 사이트 방문 이력을 수집해 사용자의 행동 패턴을 파악하는 데 이용되는 액세스한 사이트 외 쿠키 전송(서드파티 쿠키)에 제약이 있다.

쿠키와 같은 구조를 XMLHttpReqeust로 실현했다.

스크립트 태그를 삽입해야 하거나 사이트에 설치하는 데는 IFRAME의 광고보다도 시간이 걸리지만, 퍼스트파티 쿠키와 XMLHttpRequest를 조합해 제한을 극복하고 있다.

## 지오로케이션

물리적 위치를 측정하는 데는 클라이언트 자신이 측정해서 서버에 보내는 방법과 서버가 클라이언트의 위치를 추측하는 방법의 두가지가 있다.

### 클라이언트 자신이 위치를 구하는 방법

스마트폰이라면 내장된 GPS나 기지국 정보를 활용해 위치 정보를 알려줄 수 있다.

GPS가 없는 컴퓨터라도 와이파이 등을 이용한 위치 측정으로 대략적인 위치를 추측한다.

위치 정보는 사용자가 허락한 경우에만 위치 정보를 사용할 수 있다.

와이파이 자체에는 GPS가 없으므로 와이파이에서 위치 정보를 알아내는 것은 조금 교묘하고 대규모 방식으로 이루어진다.

우선 와이파이 액세스 포인트의 고유 ID(BSSID)와 위도 경도 정보를 DB로 사전에 구축해두어 클라이언트는 OS의 API을 이용해 현재 자신이 액세스 할 수 있는 액세스 포인트의 BSSID를 가져와 서버에 문의해 위도와 경도를 조회한다.

BSSID는 스마트폰이나 컴퓨터에서 와이파이를 선택할 때 보이는 이름(이른바 SSID, 정확히 ESSID)과 다르다.

BSSID는 와이파이 기기의 식별자의 48비트 수치로, 기기마다 독특한 수치로 되고 있다.(맥 주소와 같은 것)

### 서버가 클라이언트 위치를 추측하는 방법

또 한 가지 방법이 지오(Geo)IP라고 불리는 IP 주소로 추측하는 방법

## **X-Powered-By** 헤더

서버가 브라우저에 응답할 때에 부여하는 헤더에 **X-Powered-By**라는 헤더가 있다.

숨길 때의 장점으로 꼽혔던 것은 응답 크기가 작아진다는 것과 서버 이름으로 특정 보안이 뚫릴 확률이 낮아진다는 점에서 이 헤더를 숨겨야하는지 논란이 일어났다.

이 헤더를 서버가 표시하는 데 따른 보안상의 우려는 **RFC 1945**에도 나와 있으며, OS 버전 등 서버 이름 이외의 불필요한 정보가 들어가서는 안 된다고 되어 있다.

서버 개발자는 헤더의 표시와 숨김을 전화할 수 있게 구현하는 것이 좋다.

HTTP/1.1을 정리해서 재정의한 RFC-7231에는 상기 모질라의 코드처럼 호환성 문제 문제를 피하기 위해 이 헤더를 사용할 수 있다고 추가 됐다.

파이프라이닝은 하위 호환성이 약한 새 기능이라서 서버 이름으로 검증할 필요가 있었다.

더 대규모로 변경한 HTTP/2나 그 전신인 SPDY는 앞장에서 소개한 대로 TLS 에 프로토콜 니고시에이션 기능을 추가하고 HTTP/1.1과 같은 계층에서 통째로 프로토콜을 전환하는 방법을 도입했다.

## 원격 프로시저 호출

원격 프로시저 호출(remote procedure call RPC)이란 것은 다른 컴퓨터에 있는 기능을 마치 자신의 컴퓨터 안에 있는 것처럼 호출하고, 필요에 따라 반환값을 받는 구조다.

원격 메서드 호출(romote method invocation RMI)이라고 불리는 경우도 있다.

RPC에는 다양한 방식이 있다. 인터넷의 확산과 함께 HTTP를 기반으로 하는 RPC가 몇 종류 등장했다.

### XML-RPC

최초로 규격화된 것이 XML-RPC이다.

전송에 사용하는 메서드는 `POST` 이고 호출하는 인수와 반환값 모두 XML로 표현하므로, `Content-Type` 은 항상 `test/xml` 이다.

`GET` 으로 캐시될 가능성이 있으므로 RPC 통신에는 적합하지 않다.

XML-RPC는 RPC이면서도 통신 내용이 플레인 텍스트이므로, 개발자가 특별한 도구를 사용하지 않아도 읽을 수 있는 것이 특징이다.

파이썬은 표준 라이브러리로 XML-RPC의 서버와 클라이언트 작성을 모두 지원한다.

### WebDAV

WebDAV는 HTTP/1.1에 포함되진 않지만, 이 시기에 만들어졌고 수많은 환경에서 지원되고 있으므로 여기에서 소개한다.

WebDAV는 HTTP를 확장해 분산 파일 시스템으로 사용할 수 있게 한 기술이다.

WebDAV의 용어를 정리한다.

- 리소스
    - 일반 파일 시스템에서는 데이터를 저장하는 아토믹 요소를 `파일` 로 부르지만, WebDAV에서는 HTTP 용어를 그대로 이어받아 `리소스`로 부른다.
- 컬렉션
    - 폴더와 디렉터리에 해당하는 요소이다.
- 프로퍼티
    - 리소스와 컬렉션이 가질 수 있는 추가 속성이다.
    - 작성일시와 갱신일시, 최종 갱신자와 같은 정보가 해당한다.
- 락
    - 분산 파일 시스템은 같은 폴더를 여러 사람이 동시에 보고 데이터를 공유할 수 있지만, 같은 파일을 동시에 편집하게 될 경우가 있다.
    - 같은 파일에 여러 사람이 동시에 기록하면 마지막에 전송된 내용 이외에는 지워져버린다.
    - 이를 피하기 위해 먼저 선언한 사람 이외의 변경을 거절하는 시스템이다.

기본 조작으로서 `COPY`와 `MOVE` 가 추가됐다.

모두 `GET` 하고 나서 `POST` (`MOVE` 의 경우는 그다음에 `DELETE`) 하면 에뮬레이션할 수 있지만, 

예를 들면 동영상 소재로 10GB 분량의 콘텐츠가 있을 때 전체를 일단 로컬에 저장했다가 다시 업로드하는 것은 비효율적이다.

`POST` 메서드는 리소스만 작성할 수 있으므로, 컬렉션을 작성하는 `MKCOL` 메서드가 추가됐다.

콜렉션 내의 요소 목록은 프로퍼티 취득하므로 `PROPFIND` 메서드를 사용한다.

`LOCk/UNLOCK` 메서드로 파일 잠금 여부를 제어한다.

현재 온라인 스토리지 서비스와 WebDAV가 다른 점은 동기를 전제로 한다는 것이ㅏㄷ.

네트워크가 없으며 파일 목록을 가져올 수 조차 없다.

한편 드롭박스 등은 로컬에 사본을 두고, 필요할 때 동기하는 구조다.

WebDAV는 구조상 어쩔 수 없이 로컬 파일 시스템보다 성능이 떨어진다.

나중에 동기화하는 타입이라면 체감 속도면에서 로컬 드라이브와 손색이 없고, 저장 공간이 한정된 모바일 기기에서는 동기형으로 파일 액세스 할 수 있는 애플리케이션이 제공되면서 필요한 파일만 받는 시스템도 제공된다.

현재 오픈 소스 개발에서 가장 많이 사용되는 버전 관리 시스템인 Git에서는 전송용 프로토콜로 SSH와 HTTPS 두 가지를 지원한다.

사실은 이 HTTPS 안에서는 WebDAV를 사용한다

SSH는 암호화된 통신 경로를 사용하지만, 그 안의 통신은 오리지널 Git 프로토콜을 사용한다.

HTTPS라면 어느 WebDAV 서버를 사용해도 호스트할 수 있으므로 설정이 간단하지만 차문만 전송할 수 있는 Git 프로토콜 쪽이 통신 속도는 뛰어나다.

## 웹 사이트 간 공통 인증 및 허가 플랫폼

인터넷의 보급과 함께 여러 웹 서비스가 등장했다.

사용자는 웹 서비스마다 메일 주소, 사용자 ID와 패스워드를 입력해서 계정을 만들어야 했다. 다양한 이유에서 이런 과정을 간편하게 만드는 시스템을 갖추려는 활동이 조직의 벽을 넘어 이루어졌다.

지금은 1Password나 키패스 같은 패스워드 관리 도구가 널리 이용되고, 서비스마다 다른 패스워드를 제공하는 기술도 계속 보급되고 있다.

**인증(authentication)**

로그인하려는 사용자가 누구인지 확인한다. 브라우저를 조작하는 사람이 서비스에 등록된 어느 사용자 ID의 소유자인지 확인한다.

**권한 부여 (authorization)**

인증된 사용자가 누구인지 파악한 후, 그 사용자게 어디까지 권한을 부여할지 결정한다.

### 싱글 사인온

기업 내에서 사용하는 웹 서비스나 시스템이 많아지면, 싱글 사인온(single-sign-on SSO)이 검토된다.

싱글 사인온은 시스템 간 계정 관리르 따로따로 하지 않고, 한 번의 로그인을 전 시스템에 유효하게 하는 기술이다. 

현재도 기업 내에서 널리 사용되고 있다.

싱글 사인온은 다른 기술과 달리 프로토콜이나 정해진 규칙이 아니고, 이런 용도로 사용되는 시스템을 가리키는 명칭이다.

구현 방식도 웹 에만 한정되지 않는다.

싱글 사인온을 실현하는 데는 몇 가지 방법이 있다.

각 서비스가 인증 서버에 직접 액세스하러 가는 방법.

사용자 ID와 패스워드를 서비스마다 입력해야 하므로 싱글 사인온은 아니지만, 사용자 ID를 일원화해 관리할 수 있게 된다.

각 애플리케이션은 인증 시스템에 로그인하는 과정을 대행한다. 

그밖에는 티켓을 이용한 방법이 있다.

웹 서비스로 한정되지만, 각 서비스의 앞단에 HTTP 프록시 서버를 두고 인증을 대행하는 방법과 각 서비스에 인증을 대행하는 에이전트를 넣고 로그인 시 중앙 서버에 액세스해 로그인됐는지 확인하는 방법도 있다.

### 커베로스 인증

인터넷보다도 이전 시대부터 내려온 방법으로는 본래의 사용자 관리 구조를 하나로 정리해, 모든 시스템에서 이용하는 방법이 있다.

공통 규격으로서 기억 내에서도 많이 이용되는 것이 RFC 2251에 정의된 LDAP(Lightweight Directory Access Protocol)이다.

OpenLDAP, 액티브 디렉터리(Active Directory AD) 같은 구현이 있다.

**LDAP**

원래 싱글 사인온을 위한 시스템이 아니라, 이용자, 조직, 서버 등 기억 내 정보를 일원화해 관리하는 데이터베이스다.

v3에서 추가된 SASL(Simple Authentication and Security Layer)이라는 인증 기능과 세트로 기업 내 마스터 인증 시스템으로 사용된다.

RFC 1510(최신은 RFC 4129)에 정의된 커베로스(Kerberos) 인증이 널리 사용된다.

커베로스 인증을 하면, 티켓 보증 서버로의 액세스 토큰인 티켓 보증 티켓(ticket granting ticket TGT)과 세션 키를 얻을 수 있다.

서비스와 시스템을 사용할 때는 TGT와 세션 키를 티켓 보증 서버(ticket granting server TGS)에 보내고, 클라이언트에서 서버로 액세스하기 위한 티켓과 세션 키를 받는다.

이들은 서비스가 가진 비밀 키로 암호화되어 있다.

사용자가 서비스를 사용할 때는 이 티켓과 세션 키를 서비스에 보냄으로써 싱글 사인온이 실행된다.

### SAML

SAML(Security Assertion Markup Language)은 웹 계통의 기술(HTTP/SOAP)을 전제로 한 싱글 사인온 구조다.

옥타, 원로그인 같은 SaaS(software as a service) 형태로 제공되는 서비스도 있다.

SAML은 XML 기반의 표준을 많이 다루는 OASIS(Organization for the Advancement of Structured Information Standards)에서 책정된 규정이다.

SAML은 쿠키로 세션을 관리하는 웹의 구조를 따르고, 도메인을 넘어선 서비스 간 통합 인증을 할 수 있다.

서비스 간 정보 교환 메타데이터도 공통화됐다.

**사용자**

- 브라우저를 조작하는 사람

**인증 프로바이더(IdP)**

- ID를 관리하는 서비스

**서비스 프로바이더(SP)**

- 로그인이 필요한 서비스

구현 방법은 여섯가지가 있다. 이 절에서는 HTTP POST 바인딩 구현을 소개한다

- SAML SOAP 바인딩
- 리소스 SOAP(PAOS) 바인딩
- HTTP 리디렉트 바인딩
- HTTP POST 바인딩
- HTTP 아티팩트 바인딩
- SAML URI 바인딩

우선 사전 준비로 인증 프로바이더에 서비스 정보를 등록한다.

등록할 대는 메타데이터로 불리는 XMl 파일을 사용한다.

여기에 XML 파일을 인용하진 않지만 다음과 같은 항목이 포함된다.

- 서비스 ID(인증 프로바이더가 서비스를 식별하기 위한 것)
- 인증 프로바이더가 HTTP-POST할 엔드포인트 URL
- 바인딩
- 경우에 따라서는 X.509 형식의 공개 키

서비스 프로바이더 쪽에도 인증 프로바이더 정보를 등록한다.

등록할 정보는 인증 프로바이더가 XMl 파일로 제공한다.

여기에도 일련의 통신에서 사용할 엔드포인트 URL 목록과 인증서가 포함된다.

일련의 프로세스 사이에 신뢰 관꼐가 맺어져 있는 것이 전제다.

실제 통신은 단순하다.

사용자가 서비스를 이용하려고 접속했고, 아직 로그인을 하지 않았따면

서비스 프로바이더는 인증 프로바이더로 리디렉트한다.

브라우저는 인증 프로바이더 화면을 표시한다.

그 화면에서 로그인에 성공하면 인증 프로바이더는 로그인 정보를 서비스 프로바이더로 `POST` 한다.

이때도 자동으로 `POST` 하는 HTMl을 반환한다.

이제 서비스 프로바이더는 사용자가 로그인에 성공한 것을 알 수 있으므로, 처음에 사용자가 요청한 페이지의 콘텐츠를 보여준다.

### 오픈아이디

오픈아이디(OpenID)는 중앙 집중형 ID 관리를 하지 않고, 이미 등록된 웹 서비스의 사용자 정보로 다른 서비스에 로그인할 수 있는 시스템

현재는 후속 기술인 오픈아이디 커넥트를 이용하는 웹사이트는 늘고 있고, 오픈아이디로 이용할 수 있는 서비스는 줄고 있다.

오픈아이디 고유 용어가 몇 가지 등장한다.

**오픈아이디 프로바이더(OpenID provider)**

사용자 정보를 가진 웹 서비스.

사용자는 이미 이 서비스의 ID가 있다.

2017년에 야후! 재팬과 하테나가 지원한다.

**릴레잉 파티(relying party)**

사용자가 새로 이용하고 싶은 웹 서비스.

2017년 이용할 수 있는 서비스는 이벤트 등록과 참가를 관리하는 웹 서비스인 ATND가 있다.

**사용자 입력 식별자**

사용자가 입력할 URL형식으로 된 문자열.

오픈아이디 프로바이더가 제공하며, 오픈아이디 프로바이더의 사용자 프로필 화면 등에 표시된다.

사용자를 판별하는 ID가 아니라 서비스명 수준의 식별자가 사용되기도 한다.

야후! 재팬과 하테나에 계정이 있는 사용자가 그 인증 정보로 ATND를 이용할 수 있다

사용자의 조작 순서는 다음과 같다

1. 우선 오픈아이디 프로바이터 웹사이트에서 사용자 입력 식별자를 구한다.
2. 릴레잉 파티 웹사이트를 브라우저로 열고, 로그인 페이지에서 오픈아이디 로그인을 선택. 오픈아이디의 사용자 입력 식별자를 릴레잉 파티 오픈아이디 입력란에 등록
3. OpenID 프로바이더의 웹사이트로 리디렉트된다. 허가를 요청하므로 승인한다
4. 릴레잉 파티 웹사이트로 돌아온다. 릴레잉 파티 서비스를 이용할 수 있게 되며, 추가 정보가 필요할 때는 추가 정보 입력 화면이 표시된다.

릴레잉 파티는 입력된 사용자 입력 식별자를 오픈아이디 프로바이더로 넘겨주고 결과만 받아온다.

릴레잉 파티는 사용자가 오픈아이디 프로바이더에 저장한 사용자 ID나 패스워드에 접근할 일이 없다.

2번 3번 사이에서 릴레잉 파티가 오픈아이디 프로바이더와 HTTP로 정보를 교환할 비밀키를 공유하며, 릴레잉 파티에서 오픈아이디 프로바이더로 갔다가 오픈아이디 브로바이더에서 릴래잉 파티로 돌아올 때 HTTP 리디렉트를 사용한다.

오픈아이디는 릴레잉 파티를 사용하려는 사용자가 다른 서비스에서 인증됐음을 전달한다.

오픈아이디 프로바이더에 따라서 ID 등 추가 정보를 릴레잉 파티에 제공하기도 한다.

또한, 릴레잉 파티용으로 발행되는 전용 오픈아이디 식별자는 매번 바뀐다.

오픈아이디 프로바이더 화면에서 인증한 릴레잉 파티의 목록을 확인하거나 인증을 취소할 수 있다.

### 오픈소셜

오픈소셜은 회원 정보나 친구 관계를 가져오는 Person&Friend API, 액티비티를 작성하는 Activites API, 정보를 저장하거나 공유하는 Persistence API, 다른 멤버에게 메시지를 보내는 requestSendMessage 등 다양한 기능을 지원하며 인증에만 머무르지 않고 플랫폼을 지향한다.

오픈아이디의 릴레잉 파티의 경우, 서비스 쪽 제약은 거의 없고 오픈아이디 프로바이더와 릴레잉 파티를 사전에 등록할 필요도 없다.

오픈소셜의 경우, 오픈아이디 프로바이더에 해당하는 부분이 소셜 네트워크 서비스 제공자다.

인증과 권한 부여를 모두 이쪽에서 한다.

릴레잉 파티인 서드 파티 애플리케이션은 서버를 SNS 외부에 준비하지만, UI 부분은 가젯으로 불리며 미리 정해진 규칙에 따라 XMl 파일을 만들고, 친구 목록을 가져오거나 하는 각종 API를 이용해 자바스크립트와 AJAX로 애플리케이션을 개발한다.

브라우저의 관점, 즉 HTTP 차원에서 보면 오픈소셜을 사용하는 소셜 인터넷 워크 서비스에 평소처럼 로그인할 뿐 특별한 기능은 사용하지 않는다.

HTML의 IFRAME을 사용해 가젯이라는 서드파티 앱에 액세스하지만, 브라우저에서 사용하는 자바스크립트의 AJAX API나 도메인 외부 서버에 대한 요청 등은 이 가젯 서버에서 대응한다.

가젯 제공자와 플랫폼 제공자 사이의 통신은 HTTP 그 자체다.

가젯 설정 파일인 XML은 외부 서버에 두고, 가젯 서버가 HTTP로 가져온다.

클라이언트 브라우저의 요청도 가젯 서버가 중계해서 HTTP로 외부 서버에 도달한다.

외부 서버가 사용자 정보를 취득하려면 RESTful API에 HTTP로 액세스한다.

### OAuth

OAth는 인증이 아니라 권한을 부여하는 시스템으로서 개발됐다.

지금까지 소개한 것은 모두 `사용자가 누구인가`를 판정하는 인증 프로세스였다.

OAuth는 인증이 아니라 권한 부여에 특화된 시스템이다.

**권한 부여 서버**

오픈아이디에서 말하는 오픈아이디 프로바이더, 사용자는 이 권한 부여 서버의 계정이 있다.

**리소스 서버**

사용자가 허가한 권한으로 자유롭게 액세스할 수 있는 대상.

트위터나 페이스북의 경우는 권한 부여 서버와 같다.

**클라이언트**

오픈아이디에서 말하는 릴레잉 파티.

사용자가 이제부터 사용할 서비스나 애플리케이션.

오픈아이디와 달리 권한 부여 서버에 애플리케이션 정보를 등록하고, 전용 ID(client_id와 client_secret)를 가져와야한다.

이 ID를 크레덴셜이라고 부르기도 한다.

오픈아이디와 OAuth 모두 비슷하게 화면을 전환한다.

사용자가 새로운 웹 서비스를 이용하려고 할 떄, 이미 계정이 있는 서비스(트위터나 페이스북)의 웹 사이트로 전환된다.

바뀐 화면에서 사용자가 승인 버튼을 누르면, 처음에 연 웹 서비스 화면으로 다시 돌아와 정상적으로 서비스를 이용할 수 잇게 된다.

클라이언트가 사용자의 ID와 패스워드를 건들지 않는 것도 같다(예외도 있음)

OAuth는 실제로 클라이언트=외부 웹 서비스에 무엇을 허가할지는 **범위**로 결정된다.

OAuth도 오픈아이디처럼 나중에 권한 부여 서버의 설정 화면에서 허가를 취소하거나 허가 범위를 변경할 수 있다.

### 오픈아이디 커넥트

오픈아이디 커넥트(OpenDI Connect) 는 OAuth 2.0을 기반으로 한 권한 부여뿐만 아니라 인증으로 사용해도 문제가 없게 확장한 규격이다.

클라이언트 입정에서 볼 때 OAuth 2.0과의 차이는 사용자 프로필에 액세스하는 방법을 규격화한 점이다.

규격화되어 어느 서비스나 같은 방법으로 오픈아이디 프로바이더에 사용자 ID 같은 정보를 요청하고 인증에 사용할 수 있게 됐다.

일반 액세스 토큰과 별개로 ID 토큰이 발행되는데, 이 토큰을 사용해 액세스할 수 있다.

사용자 관점에서의 절차는 오픈아이디, OAuth와 같다.

오픈아이디 커넥트에서는 액세스 토큰과 ID 토큰을 가져오기 위해 두 개의 엔드포인트와 세개의 플로를 정의했다.

**권한 부여 엔드포인트**

클라이언트가 권한 부여 요청을 보낼 서비스 창구.

클라이언트 인증을 하는 플로에서는 토큰 엔드포인트에 액세스 하기 위한 키(권한 부여 코드)를 반환.

인증하지 않는 플로에서는 액세스 토큰과 ID 토큰은 이 엔드포인트가 반환한다.

**토큰 엔드포인트**

액세스 토큰과 ID 토큰을 반환하는 창구.

클라이언트를 인증해 강한 권한을 가진 토큰을 반환.

세 개의 플로는 다음과 같다.

**Authorization Code Flow**

OAuth의 Authorization Code와 같다.

client_secret을 은닉할 수 있는 서버 환경용.

권한 부여 엔드포인트에 액세스해서 권한 부여 코드를 가져온 후, 톸느 엔드포인트에 액세스해서 토큰을 얻는다.

클라이언트 인증을 사용할 수 있으므로 가장 강한 권한을 허용할 수도 있다.

**Implicit Flow**

OAuth의 Implicit Grant와 같다.

HTML 상의 자바스크립트 등 client_secret을 은ㄴ기할 수 없는 클라이언트 환경용.

권한 부여 엔드포인트에 액세스해 코드와 토큰을 한 번에 가져온다.

**Hybrid Flow**

Implicit Flow와 비슷하지만 권한 부여 엔드포인트에서 통신에 필요한 토큰과 추가 정보를 얻기 위한 권한 부여 코드를 얻을 수 있다.

이 권한 부여 코드를 사용해 토큰 앤드포인트에 액세스 한다.

OAuth 2.0에서는 Implicit Grant로 사용자 인증을 하려고 하면, 보안에 큰 이슈가 있어 해시 코드를 사용해 각종 토큰을 검증할 수 있게 개량함.

OAuth 2.0과의 차이는 Hybrid Flow가 가장크다.

OAuth가 목표로 한 것은 다음 3자간의 워크 플로다.

- 사용자
- 클라이언트
- 권한 부여 서버

모바일 애플리케이션이 보급되면서 클라이언트가 다시 둘로 나뉘어 4자간의 워크 플로우가 필요해졌다.

- 사용자
- 클라이언트 1: 스마트폰 단말의 애플리케이션
- 클라이언트 2: 백엔드 웹 서비스
- 권한 부여 서버

Hybrid Flow는 클라이언트 1에서 client_secret을 은닉할 수 없는 환경을 위한 Implict Flow로 권한을 부여한 후에, 클라이언트 2가 클라이언트 인증을 하면서 토큰의 엔드포인트에 요청을 보내 더 강력한 권한을 가진 토큰을 얻을 수 있게 되었다.

---
# 7장. HTTP/2의 신택스: 프로토콜 재정의

이 장에서는 HTTP/2나 그 전후에 책정된 새로운 세대의 다양한 프로토콜에 관해 소개한다.

## HTTP/2

- 스트림(1.1의 파이프라인에 가까운 것)을 사용해 바이너리 데이터를 다중으로 송수신 하는 구조로 변경했다.
- 스트림 내 우선 순위 설정과 서버 사이드에서 데이터 통신을 하는 서버 사이드 푸시를 구현했다.
- 헤더가 압축되게 되었다.

단 지금까지 소개했던 메서드, 헤더, 스테이터스 코드, 바디라는 `HTTP가 제공하는 네 개의 기본 요소` 는 바뀌지 않는다.

HTTP/2의 프로토콜을 직접 읽고 쓰는 클라이언트 코드나 서버에는 큰 변경이 있지만, 통신 애플리케이션에서 보면 차이가 없다.

HTTP/2의 목적은 통식 고속화뿐이다.

프로토콜의 개선 사항

- 캐시(max-age): 통신 자체를 취소
- 캐시(ETag, Date): 변경이 없으면 바디 전송을 취소
- Keep-Alive: 액세스마다 연결에 걸리는 시간(1.5TTL)을 줄임
- 압축: 응답 바디 크기 절감
- 청크: 응답 전송 시작을 빠르게 함
- 파이프라이닝 통신 다중화

HTTP/2에서는 그동안 손보지 못한 헤더부 압축이나 규격화됐지만 여전히 활용되지 않은 파이프라이닝을 대체하는 구현이 추가됐다.

HTTP/2은 이전까지와는 완전히 다른 프로토콜이므로, 반대로 하위 호환성 문제가 일어나기 어렵다.

HTTP의 텍스트 프로토콜 내부에서의 버전 전환이 아니라, TLS에 만들어진 프로토콜 선택 기능(4장에서 소개한 ALPN)을 사용해, 통째로 통신 방식을 전환하게 되어있다.

HTTP/1.1과는 전혀 다른 프로토콜로 취급되므로 파이프라이닝과 같은 문제가 생길수 없다.

### 스트림을 이용한 통신 고속화

HTTP/2의 가장 큰 변화는 텍스트 기반 프로토콜에서 바이너리 기반 프로토콜로 변화했다는 점이다.

HTTP/2에서는 하나의 TCP 접속안에 스트림이라는 가상의 TCP 소켓을 만들어 통신한다.

스트림은 프레임에 따른 플래그로 간단히 만들고 닫을 수 있는 규칙으로 되어 있고, 일반 TCP 소켓과 같은 핸드셰이크는 필요없다.

그래서 ID값과 TCP 통신 용량이 허락하는 한, 손 쉽게 몇 만번의 접속이라도 병렬화할 수 있다.

각 접속 상태의 스테이트 머신(state machine)이 정의 되어 있으며, 비슷한 모델로 되어 있는 것을 알 수 있다.

TCP는 닫힌 상태로 LISTEN하다가, 클라이언트에서 접속 요청이 있으면 비로소 ESTABLISH(통신 가능) 상태가 된다.

HTTP/2의 스트림은 처음부터 LISTEN과 거의 같은 IDLE 상태이고, 헤더를 받으면 즉시 통신 가능한 OPEN 상태가 된다.

즉 통신할 수 있을 때 까지의 단계가 줄어든다.

### HTTP/2의 애플리케이션 계층

메서드와 경로, 헤더. 바디, 스테이터스 코드라는 네 가지 기본 요소가 존재하는 것은 변함 없다.

하지만 메서드와경로, 스테이터스 코드, 나머지 스테이터스 코드에 포함되어 있던 프로토콜 버전은 모두 유사헤더 필드화되어 헤더 안에 들어간다.

상위 애플리케이션에서 본 기능성에는 변경이 없지만, 구현상으로는 `헤더` 와 `바디` 만 있다.

HTTP/2는 바이너리화 되어, 처음에 프레임 크기가 들어간다. TCP 소켓 레이어에서는 데이터를 프레임 단위로 쉽게 분리할 수 있으므로, 수신 측 TCP 소켓의 버퍼를 빠르게 비울 수 있고, 통신 상대에게 다음 데이터를 고속으로 요청할 수 있다.

바디는 Content-Length로 크기가 유일하게 정해지는 경우가 많고 청크 형식의 경우도 크기가 적혀 있기 때문에, 하나의 요청에 대한 로드 처리 비용은 별로 다르지 않다. 그러나 요청이 여러 개 일때는 이야기가 달라진다.

HTTP/1.1에서는 청크 형식이라고 해도 하나의 요청 중에 다른 요청을 처리할 수 없었다.

여섯 개의 TCP 세션으로 처리하더라도 무거운 응답이 여섯 개 있으면, 다른 통신을 전혀 할 수 없다.

HTTP/2에서는 청크가 프레임으로 분할되어 있고 프레임끼리는 독립적이므로, 중간에 다른 프레임이 끼어들어도 문제없다.

### 플로컨트롤

HTTP/2는 인터넷 4계층 모델 중 애플리케이션 층에 해당하지만, 트랜스포트 층에 가까운 것을 내부에 가지고 있는 게 특징이다.

플로 컨트롤로 TCP 소켓과 거의 같은 기능을 구현했다.

TCP 소켓과 HTTP/2 스트림의 관계는 OS 스레드와 그린 스레드와 비슷하다고 할 수 있다.

플로 컨트롤은 스트림을 효율적으로 흐르게 하려고 이용되는 통신량 제어 처리다.

통신 속도가 지나치게 차이 나는 기기의 조합으로 통신할 떄 빠른 쪽이 느린 쪽에 대량으로 패킷을 보내버려 처리할 수 없게 되는 사태를 방지하는게 목적이다.

이를 실현하고자 구체적으로는 통신하는 곳의 윈도우 크기(받아들일 수 있는 빈 버퍼크기) 관리를 사용한다.

송신하는 쪽은 상대방의 최대 버퍼 크기만큼 데이터를 보낸다.

수신하는 쪽에서 전송된 패킷을 처리하고 버퍼에 여유가 생기면, `WINDOW_UPDATE` 프레임을 이용해 새로 생긴 여유 버퍼 크기를 송신하는 쪽에 반환한다.

송신하는 쪽에서 이 통지를 받으면, 새로 생긴 여유 버퍼를 채울 만큼 이어지는 데이터를 보낸다.

`SETTINGS` 프레임을 사용하면, 초기 윈도우 크기, 최대 병렬 스트림 수, 최대 프레임 크기, 최대 헤더 리스트 크기와 같은 속도에 관련된 매개변수를 조정할 수 있다.

### 서버 푸시

시맨틱스를 보면 HTTP/1.1과 HTTP/2는 거의 같지만, 서버 푸시로 불리는 기능만은 다르다.

서버 푸시를 이용해 우선순위가 높은 콘텐츠를 클라이언트가 요구하기 전에 전송할 수 있게 됐다.

단 웹소켓처럼 양방향 통신을 실현하는 기술과 달리, 어디까지나 CSS와 자바스크립트, 이미지 등 웹페이지를 구성하는 파일을 다운로드하는 용도로 이용된다.

클라이언트가 요청을 보낼 때까지 데이터가 서버 쪽에서  푸시된 것을 감지할 수는 없다.

푸시된 콘텐츠는 사전에 캐시에 들어간다.

콘텐츠가 캐시에 들어간 뒤에 클라이언트가 그 파일을 요청하면, 곧 바로 다운로드할 수 있는것 처럼 보인다.

### HPACK을 이용한 헤더 압축

헤더는 HPACK이라는 방식으로 압축된다.

압축 알고리즘은 대부분 데이터 압축시에 사전과 사전의 키 배열이라는 두 가지 데이터를 만든다.

같은 길이의 문장이 많으면 많을수록 사전의 항목은 적어지고, 같은 키가 여러 번 사용되면 압축률은 올라단다.

HPACK은 일반적으로 사용되는 파일 압축 알고리즘과 달리, 사전에 사전을 가지고 있다.

HTTP 헤더에서는 정해진 이름이나 결과가 자주 출현하므로, 이를 외부 사전에 넣어두면 압축 후 크기가 작아진다.

HTTP/2에서는 정적 테이블(static table)이라는 이름으로 사전에 빈번하게 출현하는 헤더 이름과 헤더 값을 테이블로 가지고 있다.

추가로 같은 커넥션에서 등장한 HTTP 헤더는 인덱스화되어 동적 테이블에 저장된다.

다시 등장할 때는 인덱스 값만으로 표현할 수 있으므로 작은 크기로 송신할 수 있다.

### SPDY와 QUIC

`SPDY`는 구글이 개발한 HTTP대체 프로토콜로, 거의 그대로 HTTㅎP/2가 되었다.

구글이 `SPDY` 를 개발했던 이유는 그동안 HTTP가 개선해온 전송 속도를 한층 더 향상시키기 위해서다.

웹사이트 구성에 따라 효과가 크게 달라지기에 도입 효과는 30%에서 세 배 이상까지 여러 수지를 들 수 있지만 병렬 접속으로 블로킹이 줄어들어 작은 파일을 많이 전송할 수록 빨라진다.

`웹사이트의 자바스크립트, CSS, 이미지 등은 가급적 적은 파일 수로 정리하는 고속화 방식` 이 HTTP/1.1 시대에 빠른 웹사이트를 만드는 일반적인 기술이었다.

`SPDY` 와 HTTP/2 이후, 파일 정리의 효과는 작아진다.

압축이 되긴 하지만, 헤더의 크기는 0이 아니고, 파일 크기가 작아질수록 패킷의 틈새가 낭비되는 경우도 증가한다.

또한 결합하는 편이 여전히 통신량은 줄어들지만, 자잘한 편이 변경이 있을 때 캐시가 살아남을 확룔이 높아진다.

`SPDY` 는 어디까지나 HTTP와 같은 층인 TCP  소켓상에 구현됐지만, 구글은 한층 더 빠르게 하려고 UDP 소켓 상에 QUIC(퀵) 이라는 프로토콜을 준비했다.

TCP는 접속 초기에 여러번 통신을 주고받을 필요가 있다.

에러를 정정하거나 순서를 정렬하기 위해 수신 통지를 반환해야 하는 등 고기능인 만큼 성능은 다소 떨어진다.

TCP와 쌍이 되는 경량 프로토콜이 UDP다. 

UDP는 TCP에서 재전송 처리, 폭주 제어 등 고급 기능을 제거해서 처음 접속할 때의 니고시에이션을 가볍게 한 프로토콜이다.

패킷 유실시 재전송 처리, 통신 경로 폭주(혼잡해서 성능이 떨어진 상태) 시 제어 등 `QUIC` 은 많은 기능을 자체적으로 구현했다.

일반적인 HTTPS 통신에서는 TCP 핸드셰이크를 실시한 후에 별도로 TLS 핸드셰이크를 할 필요가 있어, 몇 번씩 왕복하며 패킷을 교환할 필요가 있었다.

`QUIC`은 양쪽을 통합해, 더 적은 횟수의 통신으로 접속할 수 있다.

첫 접속에서도 1왕복 통신으로 니고시에이션하거나 재접속할 때는 니고시에이션 없이 `ORTT` 로 재전송할 수 있는 구조로 되어 있다.

또 스마트폰이 3G/4G 회선에서 와이파이 연결로 전환했을 때의 재접속도 원할하게 됐다.

`QUIC` 이 뛰어난 점은 재접속과 첫 통신 비용뿐만이 아니다. 

이외는 TCP와 다름없는 듯하지만, HTTP/2와 협조해 동작함으로써 두 개 층에서 중복됐던 태스크를 단순화했다.

예를 들어 거의 같은 기능이 TCP와 HTTP/2 양쪽에 있던 플로 컨트롤이 일원화됐다.

패킷 순서 정렬화에서는 애플리케이션 층을 모르는 TCP는 모든 패킷을 우직하게 정렬하지만, HTTP/2에서는 스트림 단위로 필요한 만큼 정렬한다.

`QUIC` 도 `SPDY` 처럼 이미 크롬에 포함되어 있다.

사용자가 구글 서비스를 이용할 때는 이미 요청의 절반은 `QUIC`으로 이루어진다고 한다.

## Fetch API

`Fetch API` 는 XMLHttpRequest와 마찬가지로 서버 액세스를 하는 함수다.

자바스크립트에서 이용되며, 다음과 같은 특징이 있따.

- XMLHttpRequest보다 오리진 서버 밖으로의 액세스 등 `CORS` 제어가 쉬워진다.
- 자바스크립트의 모던한 비동기 처리 작성 기법인 프로미스(Promise)를 따른다.
- 캐시를 제어할 수 있다.
- 리디렉트를 제어할 수 있다.
- 리퍼러 정책을 설정할 수 있다.
- Service Worker 내에서 이용할 수 있다.

`Fetch API` 는 캐시 등을 제어하려고 할 때 가능하다는 뜻이다. 송수신 시에 제한되는 헤더, 동일 생성원 정책의 엄격한 적용 등 보안 제한이 있어, HTTP 요청에 한정된 샌드박스라는 사실에는 변함이 없다.

브라우저에서 ssh로 외부서버에 연결되거나 Git 프로토콜을 보내거나 웹 서버를 개발하는 등의 용도로는 사용할 수 없다.

### Fetch API의 기본

`Fetch API` 에서는 보안 대책으로서 CORS를 어느 수준까지 허용할 지 나타낼 수 있다. XMLHttpRequest의 경우 모드는 변경할 수 없다.

쿠키 제한은 `credentials`에 설정한다.

웹은 속도와 보안 향상이라는 두 방향으로 발전했는데, `Fetch API`는 후자에 해당한다.

기본으로 더 엄격한 설정이 선택되어 있고, 필요에 따라 명시적으로 해제하는 설계로 되어 있다.

### Fetch API만 할 수 있는 것

**캐시 제어**

캐시를 세밀하게 제어할 수 있다.

`no-store` 와 `reload`, `no-cache`를 사용하면 , 캐시 상태와 상관 없이 강제로 요청이 발생한다.

이 가운데 `no-cache`의 경우는 캐시에 대한 정보를 보내므로, 바디가 송신되지 않은 채 `304 Not Modified` 를 수신할 가능성이 있다.

반대로 캐시를 적극적으로 사용하는 것이 `force-cache` 와 `only-if-cached` 다.

`Max-Age` 헤더로 지정한 기한이 다 되어도 적극적으로 캐시를 사용한다.

후자는 캐시가 없으면 오류가 발생하고 외부로의 요청은 일어나지 않는다.

**리디렉트 제어**

`manual` 지정 시 리디렉트가 있으면, 응답 자체가 아니라 응답을 감싸 필터링된 결과를 응답으로서 반환한다.

이 응답은 `type` 속성에 `opaqueredirect` 라는 문자열이 들어 있는 이외의 정보는 필터링되어 아무것도 얻을 수 없다.

리디렉트 도중에는 보안상 누설돼선 곤란한 URL 이나 헤더가 포함되기 때문이다.

바디는 null로 상태도 0이고 헤더도 얻을 수 없으므로, 실질적으로 `리디렉트가 있었다` 는 것 밖에 모른다.

`error`와 달리 오류가 되지 않는 정도의 의미다

**Service Worker 대응**

현재 Service Worker 내에서 외부 서비스로 접속할 때는 `Fetch API`만 사용할 수 있는 사양으로 되어 있다.

웹이 애플리케이션의 기능성을 지닐 수 있게 하려는 노력(프로그레시브 웹 앱(progressive web app)이 구글을 중심으로 이루어졌다.

그런 노력의 하나로서 애플리케이션의 생애주기와 통신 내용을 제어할 수 있게 하는 `Service Worker`가 개발됐다.

`Service Worker` 를 지원하는 웹 서비스는 오프라인으로 동작할 수 있게 되거나 통지를 다룰 수 있게 된다.

`Service Worker`는 웹 서비스의 프론트엔드 자바스크립트와 서버 사이에서 동작하는 중간 레이어다.

## server-sent events

`server-sent events` 는 HTTP5의 기능 중 하나다.

기술적으로는 HTTP/1.1의 청크 형식을 이용한 통신 기능을 바탕으로 한다.

청크 형식은 거대한 파일 콘텐츠를 작게 나누어 전송하기 위한 통신 방식이었다.

청크 형식의 `조금씩 전송`  한다는 특징을 응용해, 서버에서 임의의 시점에 클라이언트에 이벤트를 통지할 수 있는 기능을 실현했다.

HTTP는 기본적으로 클라이언트에서 요청을 서버로 보내고, 서버가 요청에 대해 응답하는 클라이언트/서버 모델이다.

통신의 시작은 클라이언트가 결정하고, 클라이언트가 1회 요청하면 1회 응답이 발생하는 것이 기본 구성이다.

서버에서 정보를 돌려보내는 방법으로는 4장에서 소개한 `코멧` 이 있다. 

클라이언트에서 정기적으로 요청을 보내 서버 이벤트를 검출(폴링)하거나 요청을 받은 상태에서 응답을 보류하는(롱 폴링) 방법이 자주 사용됐다.

아직까지 이 방법은 다른 방법을 사용할 수 없게 됐을 때의 폴백으로써 사용된다.

`server-sent events` 는 코멧의 롱 폴링과 청크 응답을 조합해, 한 번의 요청에 대해 서버에서 여러 이벤트 전송을 제공한다. 

검증된 청크 방식을 사용하므로, 프록시 지원도 포함해 하위 호환성에 문제는 없다.

`server-sent events`는 청크 방식을 사용하지만, HTTP 위에 별도의 텍스트 프로토콜을 실었다.

이는 이벤트 스트림으로 불리며, MIME 타입은 `test-event-stream` 이다.

자바스크립트에서 보이지 않는 레이어에서는 재접속이나 접속 유지를 한다.

클라이언트는 메시지의 ID를 기록하고 재접속 시에는 마지막으로 수신한 ID를 `Last-Event-ID` 헤더로서 전송한다.

서버는 헤더를 발견하면, 클라이언트가 거기까지는 수신에 성공한 것으로 판단해 이후 이벤트만을 전송한다.

## 웹소켓

웹소켓(WebSocket)은 서버/클라이언트 사이에 오버헤드가 적은 양방향 통신을 실현한다.

통신이 확립되면 서버/클라이언트 사이에서 일대일 통신을 수행한다.

프레임 단위로 송수신하지만, 상대방이 정해져 있으므로 전송할 곳에 관한 정보는 갖지 않는다.

HTTP의 기본 요소 중에서 바디만 보내는 것과 같다.

프레임은 데이터 크기 등을 가질 뿐 오버헤드도 2바이트에서 14바이트밖에 안된다.

통신이 시작되면 양쪽에서 자유롭게 데이터를 주고받을 수 있다.

### 웹소켓은 스테이트폴

웹소켓이 HTTP 기반 프로토콜과 다른 점은 `스테이트 풀 통신`  이라는 점이다.

HTTP는 속도를 위해 `Keep-Alive` 등의 복잡한 메커니즘도 갖추게 됐지만, 기본적으로 요청 단위로 접속이 끊어져도 시맨틱스 측면에서는 문제가 없다.

로드 밸런서(ELB)를 이용해 서버 여러 대에 분산해두고, 요청할 때마다 다른 서버가 응답해도 된다.

`Server-Sent Event`도 전송한 ID를 일원적으로 관리해서 보증할 수 있다면, 요청을 다루는 서버가 바뀌더라도 이상 없도록 설계되어 있다.

웹소켓을 사용할 때 서버느 메모리에 데이터를 가진 상태로 통신하는 케이스가 많을 것이다.

문자 채팅과 같은 유스케이스에선 단일 서버에 모든 브라우저가 접속할 뿐만 아니라, 멤캐시드나 레디스를 중계해서 부하를 분삲시킬 수도 있다.

하지만 여러 사람이 실시간으로 동시에 플레이하는 게임처럼 부하 분산에 따른 지연을 허용할 수 없는 경우도 있다.

그러한 경우 예를 들어 채팅이라면  `대화방` 단위로 커넥션을 온 메모리로 관리한다.

이런 경우 일단 접속이 끊어졌을 때 재접속은 이전과 같은 서버로 연결할 필요가 있어, 단순한 HTTP 기반 로드 밸런서를 사용할 수 없다.

웹소켓 운용 시에 클라이언트에서 서버를 지정해 재접속할 수 있도록 로드 밸런서를 사용하지 않은 채 운용하는 사례도 볼 수 있다.

그렇지 않으면 TCP 수준의 로드 밸런서를 구사하는 웹소켓 대응 로드 밸런서를 이용할 필요가 있다.

### 자바스크립트의 클라이언트 API

웹소켓은 HTTP의 하위 레이어인 TCP 소켓에 가까운 기능을 제공하는 API이다.

자바스크립트의 API도 TCP 소켓의 API에 가까운 형태로 되어있다.

통신은 서버가 수신을 기다리는 상태에서 반드시 클라이언트 쪽에서 접속한다.

1. 서버가 특정 IP 주소, 포트 번호로 시작한다(Listen).
2. 클라이언트(브라우저)가 서버에게 통신을 시작한다고 선언한다(Connect).
3. 클라이언트가 보낸 접속 요청을 서버가 받아들인다(Accept).
4. 서버에는 소켓 클래스의 인스턴스가 넘어온다.
5. 서버가 받아서 처리하면 클라이언트의 소켓 인스턴스는 송신 기능, 수신 기능이 활성화 된다.

자바스크립트의 웹소켓 API 이름은 이와 다르지만, 기본적인 사고 방식에는 차이가 없다.

일단 HTTP로 접속한 후 업그레이드하므로 내부 절차는 조금 복잡하지만, 외부에 보이는 일련의 과정은 같다.

클라이언트가 접속할 때 하는 일은 두 가지 뿐이다.

`WebSocket 클래스` 의 생성자로 접속할 URL을 지정하고,

`send()` 메서드로 데이터를 전송

접속 후 소켓에 대해 클라이언트에서느 ㄴ하는 조작은 다음 세가지다.

- send([데이터]): 메서드로 데이터를 서버에 송신
- onmessage: 이벤트 핸들러로 서버에 보내진 데이터를 수신
- close([코드[, 이유]]): thzpt ekerl

tntlsdpsms `onmessage` 메서드를 사용한다

`onmessage` 이벤트의 사용 방법은 `Server-Sent Event` 와 같다.

### 접속

웹소켓 통신은 4장에서 소개한 프로토콜 업그레이드를 사용한다.

우선 일반 HTTP로 시작하고 그 안에서 프로토콜을 업그레이드해 웹소켓으로 전환한다.

클라이언트에서 서버로 요청을 보내는데, `Upgrade` 헤더를 이용해 `websocket` 으로 업그레이드를 요청한다.

- Sec-WebSocket-Key: 랜덤하게 선택된 16바이트 값을 BASE64로 인코딩한 문자열
- Sec-WebSocket-Version: 버전
- Sec-WebSocket-Protocol: 이 헤더는 옵션으로 웹소켓은 단순히 소켓 통신 기능만을 제공. 그중 어떤 형식을 사용할지는 애플리케이션에서 결정. 콘텐츠 네고시에이터처럼 복수의 프로토콜을 선택할 수 있도록 사용된다.
- Sec-WebSocket-Accept: Sec-WebSocket-Key를 정해진 규칙으로 변환한 문자열. 이로써 클라이언트는 서버와의 통신 확립을 검증할 수 있다.
- Sec-WebSocket-Protocol: 클라이언트로부터 서브 프로토콜 목록을 받았을 때, 서버는 그중하나를 선택해서 반환한다. 보낸 프로토콜이외의 다른 프로토콜을 받으면 클라이언트는 접속을 거부해야만 한다.

### Socket.IO

웹소켓은 강력한 API이지만. 좀 더 쉽게 사용하게는 `Socket.IO`라는 라이브러리를 경유해 사용하는 경우가 많다.

다음은 `[Socket.IO](http://socket.IO)` 장점이다.

- 웹소켓을 사용할 수 없을 때는 XMLHttpRequest에 의한 롱 폴링으로 에뮬레이션해, 서버에서의 송신을 실현하는 기능이 있다.
- 웹소켓 단절 시 자동으로 재접속한다.
- 클라이언트뿐만 아니라 서버에서 사용할 수 있느 ㄴ구현도 있어, 클라이언트가 기대하는 절차로 폴백인 XMLHttpReuqest 통신을 핸들링할 수 있다.
- 로비 가능

웹소켓이 사용되기 시작한 애초에는 하위 호환성을 넓게 유지할 수 있다는 점에서 웹소켓은 `[Socket.IO](http://socket.IO)` 를 사용했다.

현재는 웹소켓을 사용할 수 없는 브라우저가 거의없다.

기업 내 보안 관리를 위한 웹 프록시 등 웹소켓을 사용할 수 없게 되는 요인이 몇 가지 있지만, 하위 호환성을 목적으로 다른 라이브러리를 사용할 이유는 상당히 줄었다.

재접속 처리 등 장점이 전혀 없진 않지만, 앞으로는 직접 사용되거나 사용된다고 해도 XMLHTtpRequest 폴백을 꺼두는 경우가 늘 것이다.

